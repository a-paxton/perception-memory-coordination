% Template for Cogsci submission with R Markdown

% Stuff changed from original Markdown PLOS Template
\documentclass[10pt, letterpaper]{article}

\usepackage{cogsci}
\usepackage{pslatex}
\usepackage{float}
\usepackage{caption}

% amsmath package, useful for mathematical formulas
\usepackage{amsmath}

% amssymb package, useful for mathematical symbols
\usepackage{amssymb}

% hyperref package, useful for hyperlinks
\usepackage{hyperref}

% graphicx package, useful for including eps and pdf graphics
% include graphics with the command \includegraphics
\usepackage{graphicx}

% Sweave(-like)
\usepackage{fancyvrb}
\DefineVerbatimEnvironment{Sinput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{Soutput}{Verbatim}{}
\DefineVerbatimEnvironment{Scode}{Verbatim}{fontshape=sl}
\newenvironment{Schunk}{}{}
\DefineVerbatimEnvironment{Code}{Verbatim}{}
\DefineVerbatimEnvironment{CodeInput}{Verbatim}{fontshape=sl}
\DefineVerbatimEnvironment{CodeOutput}{Verbatim}{}
\newenvironment{CodeChunk}{}{}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

\usepackage{color}

% Use doublespacing - comment out for single spacing
%\usepackage{setspace}
%\doublespacing


% % Text layout
% \topmargin 0.0cm
% \oddsidemargin 0.5cm
% \evensidemargin 0.5cm
% \textwidth 16cm
% \textheight 21cm

\title{Interpersonal Coordination of Perception and Memory in Real-Time Online
Social Experiments}


\author{{\large \bf Alexandra Paxton} \\ \texttt{paxton.alexandra@gmail.com} \\ Institute of Cognitive and Brain Sciences \\ Berkeley Institute for Data Science \\ University of California, Berkeley \And {\large \bf Thomas J. H. Morgan} \\ \texttt{thomas.j.h.morgan@asu.edu} \\ School of Human Evolution and Social Change \\ Arizona State University \AND {\large \bf Jordan W. Suchow} \\ \texttt{suchow@berkeley.edu} \\ Social Science Matrix \\ University of California, Berkeley \And {\large \bf Thomas L. Griffiths} \\ \texttt{tom\_griffiths@berkeley.edu} \\ Department of Psychology \\ University of California, Berkeley}

\begin{document}

\maketitle

\begin{abstract}
The subtle hum of interpersonal coordination that runs throughout social
communication and interaction shows how individuals can subtly influence
one another's behaviors, thoughts, and emotions over time. While the
majority of research on coordination has been done on face-to-face
interaction, recent advances in crowdsourcing have opened the
opportunity to conduct large-scale, real-time social interaction
experiments. We take advantage of these tools to explore the
interpersonal coordination in a ``minimally interactive context,''
distilling the richness of natural communication into a tightly
controlled setting to explore how people become coupled in their
perceptual and memory systems while performing a task together.
Consistent with previous work on postural sway and gaze, we find that
individuals become coupled to one another's cognitive processes without
needing to be co-located or fully interactive with their partner;
interestingly---although participants had no information about nor
direct communication with their partner---we also find hints of the
context-shaping effects that social forces can play in face-to-face
interaction.

\textbf{Keywords:}
interpersonal coordination; human communication; online experiments;
social interaction
\end{abstract}

\section{Introduction}\label{introduction}

Research on the phenomenon of \emph{interpersonal coordination} focuses
on the subtle ways in which our interactions with others directly affect
our own behaviors, feelings, and thoughts. Interest in coordination
(also known as interactive alignment, interpersonal synchrony, mimicry,
and more; see A. Paxton, Dale, \& Richardson, 2016) has surged over the
last several decades as a framework for understanding how contact with
others shapes our cognition and behavior, with much of it focusing on
how we become more similar over time during task-oriented or friendly
contexts.

A growing perspective in this area has taken inspiration from dynamical
systems theory, conceptualizing interaction as a complex adaptive system
from which coordination arises as an emergent phenomenon according to
contextual pressures (Riley, Richardson, Shockley, \& Ramenzoni, 2011).
A fundamental principle of this dynamical systems perspective holds that
coordination should not be static across contexts nor over time.
Exploring how new contexts and contextual demands---like interpersonal
conflict (A. Paxton \& Dale, 2013), friendly competition (Tschacher,
Rees, \& Ramseyer, 2014), or specialized task demands (Fusaroli et al.,
2012)---has become a central part of this perspective, laying out under
what conditions coordination disappears, increases, or demonstrates
complementary rather than synchronous in-phase patterns.

There is similar interest in comparing how coordination changes across
different behavioral or cognitive systems. Under the dynamical systems
perspective, a context's unique pressures, the resulting coordination
dynamics, and their impact on the interaction may differ over time and
across settings. For example, some of the earliest work in this subset
of coordination research has found that---during task-related
interaction---individuals tend to become more similar over time across a
variety of metrics (Louwerse, Dale, Bard, \& Jeuniaux, 2012) but that
specific kinds of coordination can differentially help or hurt outcomes
(Fusaroli et al., 2012).

Broadly, during tasks that are neutral (Shockley, Santana, \& Fowler,
2003), cooperative (Louwerse et al., 2012), or competitive (but not
conflict-driven; e.g., competitive games, Tschacher et al., 2014),
previous work broadly suggests that individuals' behavior and cognition
become more similar over time. A range of behavioral signals---both
high-level (e.g., gesture; Louwerse et al., 2012) and low-level (e.g.,
postural sway; Shockley et al., 2003)---become synchronized during
interaction. Threads throughout previous work lend support to the idea
that this occurs even when the interacting individuals are unable to see
one another (Shockley et al., 2003) or are separated in time (Richardson
\& Dale, 2005), but this idea has been relatively underexplored in
coordination research.

The systematic testing of coordination across a variety of interaction
contexts is vital to charting its dynamical landscape. This methodical
exploration of different factors will eventually enable us to identify
control parameters and key factors of initial conditions that shape how
coordination emerges and how it impacts interaction outcomes. Doing so,
however, requires researchers on interpersonal coordination to expand
our view of experimental paradigms: Even as we continue to embrace more
complex naturalistic interactions (e.g., A. Paxton \& Dale, 2013;
Tschacher et al., 2014), we must continue to develop experimental
methods for analyzing ``minimally interactive contexts'' (Hale, Pan, \&
Hamilton, 2015)---that is, situations in which our interactions with
others are limited in behavioral channel, scope, or time---to fully map
the interaction space.

To create both fully interactive and minimally interactive paradigms, we
believe that online experiment platforms and crowdsourcing can be
powerful tools. By connecting people digitally, researchers are able
fully control the experimental experience---from deciding how much
social information partners will have about one another to establishing
which communication channels can be used to crafting interactive studies
for groups beyond the dyad. Crowdsourcing platforms like Amazon
Mechanical Turk (\url{http://www.mturk.com}) have been extensively used
as a means to collect on individuals (Buhrmester, Kwang, \& Gosling,
2011). However, by developing real-time interactive paradigms for these
platforms, researchers interested in social behavior can study
experimentally situated social processes beyond the lab while not
compromising on the richness and complexity of true interactive
contexts.

\subsection{The Present Study}\label{the-present-study}

Here, we build on a robust tradition within interpersonal coordination
that demonstrates that people become coordinated across a number of
behavioral channels, even when they have very little access to one
another. In this case, we focus on task performance within a minimally
interactive context through a real-time cooperative online
experiment---a nominal game that asks players to correctly perceive and
(under cognitive load) remember a line. Specifically, the current study
focuses on understanding how interacting individuals become entrained in
perception and memory over time, becoming a sort of ``line estimation
system'' (cf. Dale, Richardson, \& Kirkham, 2011).

We approach the current study with three main research questions. First,
we ask whether people become more coupled in their perceptual and memory
systems over time, despite the limited perceptual and social information
about their partner. In keeping with previous research, we expect to
find that individuals will start to exhibit similar behavioral dynamics
as they continue to play with one another.

Next, we investigate whether any observed coordination effects could
simply be an artifact of the joint learning context, and we hypothesize
that coordination dynamics will still be a significant influence on
players' cognitive and behavioral systems.

Finally, we look to whether any social factors might influence these
dynamics, even in this minimal context. Social factors like rapport and
affect play vital roles in face-to-face interaction during naturalistic
experiments (e.g., Tschacher et al., 2014). Although the game does not
facilitate any explicit social behaviors, we are interested to explore
whether some social influences surface as emergent effects, given their
importance to coordination research.

While a relatively simple paradigm, this research program contributes to
the theoretical landscape of interpersonal coordination research in
several ways. First, the investigation continues to expand work on
minimally interactive contexts---an underexplored avenue in
interpersonal coordination research, which often relies on fully
interactive paradigms. This work, then, expands coordination research to
explore whether coordination occurs even when interacting individuals
have access only to one impoverished channel of information available to
them. Second, we explore the degree to even low-level properties of
memory and perception become entrained, despite this minimal
information---explicitly probing questions of memory systems posed by
transactive memory (Tollefsen, Dale, \& Paxton, 2013). Finally, we
extend complex investigations of social impacts on coordination into
lower-level behavioral and perceptual channels. Taken together, the
present study aims to test several hypotheses that are circumscribed by
a host of related previous work, providing explicit tests for ideas that
are often implicitly accepted by coordination researchers.

\section{Method}\label{method}

All research activities were completed in compliance with oversight from
Committee for the Protection of Human Subjects at the University of
California, Berkeley.

\subsection{Participants}\label{participants}

\begin{CodeChunk}
\begin{figure*}[h]

{\centering \includegraphics{figs/2-col-image-1} 

}

\caption[Experiment flow]{Experiment flow}\label{fig:2-col-image}
\end{figure*}
\end{CodeChunk}

Participants (\emph{n} = 148) were individually recruited from Amazon
Mechanical Turk to participate as dyads (\emph{n} = 74). Participants
were paired with one another according to the order in which they began
the experiment. All participants were over 18 years of age and fluent
English speakers (self-reported); participation was restricted to only
recruit from participants located within the U.S. with a 95\% approval
rate for their HITs (\emph{Human Intelligence Tasks}; i.e., tasks posted
on
MTurk).\footnote{HIT approval rate is a measure of MTurk worker quality, capturing how often their work is rejected by a requester. A 95\% HIT approval rate means that only 5\% of all of their submitted HITs have been rejected.}

The experiment lasted an average of 11.69 minutes (range: 7.98---21.34
minutes). All participants were paid \$1.33 as base pay for finishing
the experiment and earned a bonus of up to \$2 for the entire experiment
based on their own mean accuracy over all trials (mean = \$1.80; range:
\$0.00---\$1.95). Participants were not aware of the value of their
earned bonus until after the entire experiment was completed.

\subsection{Procedure}\label{procedure}

All data collection procedures were run on Amazon Mechanical Turk
(\url{http://mturk.com}) using the experiment platform Dallinger
(v3.4.1; \url{http://github.com/dallinger/Dallinger}). Code for the
experiment is available on GitHub
(\url{http://github.com/thomasmorgan/joint-estimation-game}).

Each participant was individually recruited on Amazon Mechanical Turk to
play a ``Line Estimation Memory Game'' (advertisement: ``Test your
memory skills!''; see Fig. 1 for experiment flow). Upon completing
informed consent, participants were told that they would be playing a
game in which they would be required to remember and re-create line
lengths. Participants were informed that they would complete their
training trials individually and would then begin playing with a
partner. Participants were given no information about their partner
other than the guess that their partner made; no information about the
partner's identity was shared. Participants were told in advance that
they would receive a bonus for their own final guess's accuracy in the
test trials.

In each trial (i.e., each new stimulus set), participants were shown 3
red lines, each of a different length, and were asked to remember all
three of
them.\footnote{A pilot study found that participants performed at ceiling when given only 1 line to remember and recreate. Two more 2 lines were added to strictly increase the memory load, as opposed to adding difficulty in other ways (e.g., creating a moving stimulus).}
The 3 stimulus lines were displayed for 2 seconds then removed,
providing participants with a blank screen for 0.5 seconds. Participants
were then told which line to re-create (\#1, \#2, or \#3) and were then
given 1 second to submit their guess at how long the target line had
been. To do so, participants were given a blank box and used their
cursor to fill in the box with a blue line. All lines were presented
within bounded 500-by-25-pixel boxes.

During training, participants were then shown the true length of the
target line (as a grey bar above their own guess) for 2 seconds. This
was accompanied by a message telling the participant that they had
guessed correctly (``Your guess was correct!'') or incorrectly (``Your
guess was incorrect'') or that they had not submitted a guess within the
1-second time limit (``You didn't respond in time''). During training, a
guess was ``correct'' within 4 pixels of the true line length; however,
accuracy is modeled as a continuous variable (i.e., how far off the
guess was).

During testing, participants' stimulus viewing, waiting, and recreation
times remained the same as during testing, but they no longer received
feedback on guess accuracy. Instead, after both participants had
submitted their first guess, participants were shown their guess above
their partner's guess (see Fig. 1). Both participants were then asked
whether they wanted to change their own guess or to keep the guess they
had submitted.

If either participant in the dyad wanted to change their guess, that
participant was allowed to change their guess (again with a 1-second
time limit) \emph{while} still being able to view their partner's
previous guess. Participants who chose to keep their previously
submitted guess were informed that their partner chose to submit a new
guess and waited for their partner to finish. If both participants chose
to change their guess, they did so at approximately the same time---that
is, immediately after they said that they wanted to change their guess
and without being able to see their partner's decision or new guess.
After the new guesses were submitted, both participants were again shown
both guesses and were allowed to change or keep their guess. This
process continued until both participants chose to keep their guess.

Participants were informed that their final accuracy bonus would only be
calculated using their final guess. However, because they had no means
to communicate with their partner about whether each would be accepting
or changing their guesses, each participant could not have known whether
their decision to keep the guess would have been their final guess for
the trial. As a result, our statistical models use all guesses, not just
final guesses (see next section for more detail).

All dyads completed 10 training trials (alone) and 15 test trials (with
their partner). All training and test stimuli were randomly generated
for each dyad, but both participants within the dyad were given the same
stimuli. Stimuli were drawn from a uniform distribution between 1\% and
100\% (inclusive) of the total possible line length; this could have, by
chance, resulted in some relatively easier stimulus sets for some dyads,
which should be mitigated by our sample size. After participating, each
individual participant was asked to complete a series of questionnaires
about the game on a series of 1-10 Likert-style scales, including the
perceived difficulty of the task, how engaged they were in the task, and
questions about their own and their partner's cooperativeness and
trustworthiness.

\subsection{Measures and Model
Specifications}\label{measures-and-model-specifications}

For clarity, we present the measures and model specifications together.
Each measure used in one of our three models model is defined and
written in bold the first time it is presented in this section.

\subsubsection{Model 1 Specifications: Do Partners' Perception and
Memory Couple in Minimally Interacting
Contexts?}\label{model-1-specifications-do-partners-perception-and-memory-couple-in-minimally-interacting-contexts}

Our first model tested whether individuals' ratings became more similar
over time while accounting for training improvement.

To do that, we first calculated each participant's \textbf{error} for
\emph{each guess} of \emph{each trial}. Error was measured as a ratio
relative to the total possible error on a given target stimulus trial.
That is, rather than taking a given guess's error relative to the total
line length, error was calculated as the maximum \emph{possible} error.
For example, if the target stimulus was 60 units long, participants
could either under-estimate the line length by 60 or over-estimate it by
40. As such, the maximum possible error for that trial would be 60, and
the participant's error would be calculated relative to that maximum
possible error. We chose to use normalized error---rather than real
error---as a measure of performance that natively controlled for the
``possible wrong-ness'' associated with any given line.

We then quantified perceptual and memory \textbf{coordination} (or how
similar participants' perceptual and memory systems became over time) as
the cross-correlation coefficient of participants' error.
Cross-correlation---a common measure of coordination (A. Paxton \& Dale,
2013)---was calculated using all guesses across all trials within a
window of +/-5 guesses. Although cross-correlation produces information
about leading and following behavior, we have no \emph{a priori}
expectations about which of the two participants would emerge as a
leader (given they have no information about their partner nor any
assigned roles). Our first-pass analyses therefore ignore any
directionality by incorporating \textbf{absolute lag}, averaging across
the each incremental lags (i.e., leading/following in both participants'
directions).

To provide a baseline measure of \textbf{training improvement}, we
calculated the slope of each participant's normalized error over all
training trials. To account for individual differences in self-assessed
task difficulty, we used ordinal \textbf{ratings of difficulty} that
each participant gave after the task.

Our first model was a linear mixed-effects model predicting coordination
of normalized error with absolute lag and training improvement as fixed
effects, using dyad and difficulty ratings as random effects.

\subsubsection{\texorpdfstring{Model 2 Specifications: Can We Identify
Signatures of Learning \emph{and} Coordinative
Processes?}{Model 2 Specifications: Can We Identify Signatures of Learning and Coordinative Processes?}}\label{model-2-specifications-can-we-identify-signatures-of-learning-and-coordinative-processes}

During the experiment, both participants are not simply influencing one
another (as tested in Model 1)---but are also simultaneously learning to
play the game. To ensure any similarity found by Model 1 would not be
simply an artifact of both participants improving individually, we
needed to test the relation between participants' (1) adaptation to
their partner's perceptual estimation and memory and (2) own performance
changed over time. If participants were adapting along both avenues, we
could find evidence of these dual processes through differences in their
rates of adaptation over time.

To do this, we used the normalized error values (described above) to
derive two measures. To answer the latter point, we used each
participant's normalized error for \emph{each guess} in \emph{each
trial} as their \textbf{true error}---in other words, how much the
participant differed from the stimulus. To answer the former, we
calculated the absolute difference between both participant's true error
to obtain the \textbf{partner error} for \emph{each guess} in \emph{each
trial}---or how much the participant's guess different from their
partner's.

Because we are interested in understanding this process dynamically, we
captured participants' progress over time by creating a
\textbf{cumulative guess counter}, serving as a form of abstracted time
spent engaging with one another and the experiment. While the measure of
coordination in Model 1 presented a time-abstracted measure of
coordination across the entire experiment, this model provides a
snapshot of coordination in real time, measuring the learning and
coordinative processes from guess to guess.

For Model 2, we built a linear mixed-effects model predicting the
cumulative guess counter with each participant's true error, partner
error, the interaction term between the two, and training improvement
(described above) as fixed effects. We also included random effects for
participant and difficulty. (We did not include dyad as a random effect
in this model because the variance in the guesses occurred at the
participant level.)

\subsubsection{Model 3 Specifications: Do Social Factors Impact
Coordination in Minimally Interacting
Contexts?}\label{model-3-specifications-do-social-factors-impact-coordination-in-minimally-interacting-contexts}

To explore the role that social judgements can play even in minimally
interactive contexts, our final model considered how trust might impact
coordination. For this model, we captured a third measure of
coordination: the participant's willingness to change their guess, which
was captured by the \textbf{total number of guesses} that each
participant submitted in each trial.

Because participants individually chose whether to keep their previous
guess or submit a new one while being able to see their partner's guess,
we could expect that participants who trust their partner more would be
more likely to change their guess---especially if there were large
\textbf{absolute differences between the partner's first guesses}. Trust
was measured as each participant's self-reported Likert-style
\textbf{rating of their trust in their partner} (``How much do you feel
you trusted your partner's opinion during the experiment?'').

Model 3 was a linear-mixed effects model predicting the total number of
guesses in a trial with fixed effects for their trust in their partner
and for the difference in participants' first guess on that trial, while
controlling for trial number and how much they improved during their own
training. Model 3 also included participant and difficulty as random
effects.

\section{Results}\label{results}

All analyses were performed in R (R Core Team, 2016). Each model was
built according to the model specifications described above. Linear
mixed-effects models were performed using the \texttt{lme4} package
(Bates, Machler, Bolker, \& Walker, 2015) using the maximal random slope
structure for each random intercept to achieve model convergence (Barr,
Levy, Scheepers, \& Tily, 2013). All main and interaction terms were
centered and standardized prior to entry in the model, allowing the
model estimates to be interpretable as effect sizes (Keith, 2005).

While we do not have sufficient space in the current paper to provide
model specifications for each of our three models, our code is fully and
freely available in the GitHub repository for this project:
\url{http://www.github.com/a-paxton/perception-memory-coordination}.
Experiment data are available on the OSF repository for the project:
\url{https://osf.io/8fu7x/}.

\subsection{Model 1 Results: Coordinated Error over
Time}\label{model-1-results-coordinated-error-over-time}

As predicted, we find that dyads were significantly and strongly coupled
in their error ratings (\emph{\(\beta\)}=-0.43,
\emph{p}\textless{}0.0001), with no effect of training improvement
(\emph{\(\beta\)}=-0.06, \emph{p}=0.36). In other words, we find that
players were more likely to produce lines with similar errors at the
same time, even across repeated guesses within a single trial,
regardless of how well-adapted they were to the task during training.

\subsection{\texorpdfstring{Model 2 Results: Learning \emph{and}
Coordinative
Processes}{Model 2 Results: Learning and Coordinative Processes}}\label{model-2-results-learning-and-coordinative-processes}

As expected, we found that participants' rates of adaptation to their
partner significantly differed from their rates of adaptation to the
game (\emph{\(\beta\)}=-0.06, \emph{p}\textless{}0.008; see Figure 2).
In other words, this model suggests that individuals are simultaneously
engaging in both learning \emph{and} coordinative processes during the
game, becoming attuned to the learning task while also synchronizing to
one another's cognitive processes.

Aside from the main effect of partner adaptation, no other predictors
reached statistical significance (all \emph{p}s\textgreater{}0.25).

\subsection{Model 3 Results: Social Factors in Minimally Interacting
Contexts}\label{model-3-results-social-factors-in-minimally-interacting-contexts}

We found that greater trust in their partner predicted a small but
statisically significant \emph{increase} in the number of iterations of
guesses within a trial (\emph{\(\beta\)}=0.07,
\emph{p}\textless{}0.042), although we found no difference in the number
of guesses based solely on the differences in the partners' first
guesses (\emph{\(\beta\)}=0.01, \emph{p}=0.65). Ratings of partner trust
were normally distributed around a mean of 5.96 (SD: 2.24; range:
1--10). In other words, although Models 1 and 2 showed people are
getting better over time and becoming more similar across trials to
their partners, we also find that participants are more willing to
concede that their partner's guess was correct when the participant
trusted their partner---regardless of how similar or different their
first guess on the trial was.

Interestingly, we found that participants took more guesses on test
trials when they improved more in their training trials
(\emph{\(\beta\)}=0.05, \emph{p}\textless{}0.028). Because those with
the greatest training improvement would be the most poorly performing
initial players (i.e., because high-performing individuals would have a
much narrower band of possible improvement), this suggests that
poorer-performing players are more likely to divide the cognitive labor
of the task and follow the lead of their higher-performing partner.

We also saw an effect of trial (\emph{\(\beta\)}=-0.05,
\emph{p}\textless{}0.02), indicating that people changed their guesses
fewer times in each trial as the game progressed. This could be an
effect of learning (i.e., because both participants are improving and
becoming more similar from trial to trial), of experiment fatigue (e.g.,
if participants simply want to end the game more quickly), or some
combination of the two.

\begin{CodeChunk}
\begin{figure}[H]

\includegraphics{figs/unnamed-chunk-1-1} \hfill{}

\caption[Difference over time in coordinative and learning processes, or the change in guess deviation from truth (in blue) and from their partner's guess (in red) over the game]{Difference over time in coordinative and learning processes, or the change in guess deviation from truth (in blue) and from their partner's guess (in red) over the game.}\label{fig:unnamed-chunk-1}
\end{figure}
\end{CodeChunk}

\section{Discussion}\label{discussion}

Inspired by established lines of research on interpersonal coordination,
we here explored how minimally interactive contexts can shape the
emergence of interpersonal dynamics. Using an online experiment
developed to provide participants with only a single means of
communication---namely, their partner's ability to correctly perceive
and recall the length of a target line---we found evidence of
coordination of cognitive systems despite minimal social information and
context.

As expected, we found evidence for low-level perceptual and memory
coordination between players throughout the game. Congruent with
findings about postural sway (Shockley et al., 2003) and gaze
(Richardson \& Dale, 2005), the present work suggests that some
behavioral and cognitive processes can become coordinated even when
separated by time and space, given some access to the relevant process
in another person and a task-based interactive context to which that
process is essential. Moreover, learning and game performance of the
individual players were unable to completely account for the players'
similarity to one another.

Finally, like other work across the coordination research area, we found
that participants' coordination behaviors were shaped by subtle social
judgements. Within coordination research, rapport has long been upheld
as one of the important predictors of coordination (Hove \& Risen,
2009). Similarly, we find that players' decisions to change their
guesses were influenced by their self-reported ratings of their
partner's trustworthiness.

Taken together, our findings contribute to the ongoing efforts to
understand the form, function, and emergence of coordination. We were
specifically interested in pursuing three important questions around
interpersonal coordination of perception and memory: whether it emerges
during minimally interactive contexts, whether it can be distinguished
from other contemporaneous behavioral and cognitive processes, and
whether it is influenced by subtle social judgements. In addition to
these theoretical contributions, we also hope to have provided an
example of the utility of crowdsourcing platforms to investigate core
principles of interpersonal coordination and human interaction at a
larger scale without relinquishing experimental control.

\subsection{Future Directions}\label{future-directions}

Some of the questions left open by the present study may provide
interesting avenues for future work, both for better understanding some
of the effects identified here and for extending them into novel
territory.

We found that participants were more likely to change their guesses
during the game when playing with a partner who they rated trustworthy
\emph{after} the game. One plausible explanation for this behavior may
be that participants were more willing to change their own guess to be
closer to the guess provided by a partner whose perceptual and memory
capabilities they trusted. Another (not necessarily exclusive)
possibility is that feelings of trust are built throughout the game:
Since the ratings were made at the end of the experiment, it may be that
the partner's behavior throughout the interaction---\emph{aside} from
accuracy (e.g., duration of the interaction)---builds to create that
feeling of trust. Future work should explore the degree to which these
explanations may (individually or together) explain the emergent social
phenomena observed even minimally interactive contexts.

Second, we explored trustworthiness as a social construct during a task
that allowed only minimal participation between participants. The
trustworthiness measure was intentionally asked to allow participants to
answer broadly, providing a signal of the latent social information that
participants constructed when \emph{only} their partner's task-related
behavior was available to them. While we see their responses as a signal
of very subtle social forces, we readily recognize that these ratings of
trustworthiness may have been influenced by the individual's own
confidence or ability---as many social judgements are often influenced
by the assessor's own characteristics. Future work should expand on this
to explore the interplay between individual and interpersonal
assessments.

Finally, given the simplicity of this experimental design and the broad
participant pools available through crowdsourcing sites, this work could
be expanded to examine other important questions of scale in
interpersonal coordination. The majority of research on interpersonal
coordination has tended to focus on dyadic interaction, as we did here,
but many real-world social settings include more than two people.
Accordingly, any coherent theory around interaction must be able to
account for such settings. Crowdsourcing and real-time social
experiments enable researchers to control the interaction space much
more tightly, enabling the targeted focus on specific processes across a
massive potential participant population.

\section{Acknowledgements}\label{acknowledgements}

Our thanks go to Nelle Varoquaux and to the Dallinger development team
for their assistance in debugging the experiment. This work was funded
in part by DARPA Cooperative Agreement D17AC00004. This project was
funded in part by a Moore-Sloan Data Science Environments Fellowship to
AP, thanks to the Gordon and Betty Moore Foundation through Grant
GBMF3834 and the Alfred P. Sloan Foundation through Grant 2013-10-27 to
the University of California, Berkeley.

\section{References}\label{references}

\setlength{\parindent}{-0.1in} \setlength{\leftskip}{0.125in} \noindent

\hypertarget{refs}{}
\hypertarget{ref-barr2013random}{}
Barr, D. J., Levy, R., Scheepers, C., \& Tily, H. J. (2013). Random
effects structure in mixed-effects models: Keep it maximal.
\emph{Journal of Memory and Language}, \emph{63}(3), 255--278.

\hypertarget{ref-bates2015fitting}{}
Bates, D., Machler, M., Bolker, B., \& Walker, S. (2015). Fitting linear
mixed-effects models using lme4. \emph{Journal of Statistical Software},
\emph{67}(1), 1--48.

\hypertarget{ref-buhrmester2011amazon}{}
Buhrmester, M., Kwang, T., \& Gosling, S. D. (2011). Amazon's Mechanical
Turk: A new source of inexpensive, yet high-quality, data?, \emph{6},
3--5.

\hypertarget{ref-dale2011how}{}
Dale, R., Richardson, D. C., \& Kirkham, N. K. (2011). \emph{How two
people become a tangram recognition system}. Aarhus University, Aarhus,
Denmark.

\hypertarget{ref-fusaroli2012coming}{}
Fusaroli, R., Bahrami, B., Olsen, K., Roepstorff, A., Rees, G., Frith,
C., \& Tyl√©n, K. (2012). Coming to terms: Quantifying the benefits of
linguistic coordination. \emph{Psychological Science}, \emph{23}(8),
931--939.

\hypertarget{ref-hale2015using}{}
Hale, J., Pan, X., \& Hamilton, A. F. de C. (2015). Using interactive
virtual characters in social neuroscience. In \emph{Virtual reality
(vr), 2015 ieee} (pp. 189--190). IEEE.

\hypertarget{ref-hove2009s}{}
Hove, M. J., \& Risen, J. L. (2009). It's all in the timing:
Interpersonal synchrony increases affiliation. \emph{Social Cognition},
\emph{27}(6), 949.

\hypertarget{ref-keith2005multiple}{}
Keith, T. (2005). \emph{Multiple regression and beyond.} Boston, MA:
Pearson.

\hypertarget{ref-louwerse2012behavior}{}
Louwerse, M. M., Dale, R., Bard, E. G., \& Jeuniaux, P. (2012). Behavior
matching in multimodal communication is synchronized. \emph{Cognitive
Science}, \emph{36}(8), 1404--1426.

\hypertarget{ref-paxton2013argument}{}
Paxton, A., \& Dale, R. (2013). Argument disrupts interpersonal
synchrony. \emph{Quarterly Journal of Experimental Psychology},
\emph{66}(11).

\hypertarget{ref-paxton2016social}{}
Paxton, A., Dale, R., \& Richardson, D. (2016). Social coordination of
verbal and nonverbal behaviors. In P. Passos, K. Davids, \& C. J. Yi
(Eds.), \emph{Interpersonal coordination and performance in social
systems} (pp. 259--273). Abington, Oxon; New York, NY: Routledge.

\hypertarget{ref-r2016r}{}
R Core Team. (2016). \emph{R: A language and environment for statistical
computing}. Vienna, Austria: R Foundation for Statistical Computing.
Retrieved from \url{https://www.R-project.org/}

\hypertarget{ref-richardson2005looking}{}
Richardson, D., \& Dale, R. (2005). Looking to understand: The coupling
between speakers' and listeners' eye movements and its relationship to
discourse comprehension. \emph{Cognitive Science}, \emph{29}(6),
1045--1060.

\hypertarget{ref-riley2011interpersonal}{}
Riley, M. A., Richardson, M., Shockley, K., \& Ramenzoni, V. C. (2011).
Interpersonal synergies. \emph{Frontiers in Psychology}, \emph{2}, 38.

\hypertarget{ref-shockley2003mutual}{}
Shockley, K., Santana, M.-V., \& Fowler, C. A. (2003). Mutual
interpersonal postural constraints are involved in cooperative
conversation. \emph{Journal of Experimental Psychology: Human Perception
and Performance}, \emph{29}(2), 326.

\hypertarget{ref-tollefsen2013alignment}{}
Tollefsen, D. P., Dale, R., \& Paxton, A. (2013). Alignment, transactive
memory, and collective cognitive systems. \emph{Review of Philosophy and
Psychology}, \emph{4}(1), 49--64.

\hypertarget{ref-tschacher2014nonverbal}{}
Tschacher, W., Rees, G. M., \& Ramseyer, F. (2014). Nonverbal synchrony
and affect in dyadic interactions. \emph{Frontiers in Psychology},
\emph{5}, 1323.

\end{document}
