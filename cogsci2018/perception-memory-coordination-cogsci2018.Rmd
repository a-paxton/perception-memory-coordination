---
title: "Interpersonal Coordination of Perception and Memory in Real-Time Online Social Interaction"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Alexandra Paxton} \\ \texttt{paxton.alexandra@gmail.com} \\ Institute of Cognitive and Brain Sciences \\ Berkeley Institute for Data Science \\ University of California, Berkeley
    \And {\large \bf Thomas J. H. Morgan} \\ \texttt{thomas.j.h.morgan@asu.edu} \\ School of Human Evolution and Social Change \\ Arizona State University
    \AND {\large \bf Jordan W. Suchow} \\ \texttt{suchow@berkeley.edu} \\ Social Science Matrix \\ University of California, Berkeley
    \And {\large \bf Thomas L. Griffiths} \\ \texttt{tom\_griffiths@berkeley.edu} \\ Department of Psychology \\ University of California, Berkeley}
    
abstract: 
    "Recent advances in crowdsourcing have helped many cognitive scientists reach out beyond traditional undergraduate subject pools to run a range of experimental paradigms with a wider audience. To date, however, many of these opportunities for online experiments on crowdsourcing platforms have been closed to researchers interested in capturing the dynamics of human social interaction. We argue that an important next step for increasing the adoption and utility of online experiments will lie in using *networked crowdsourcing*---moving beyond providing individual participants separate tasks to support more complex interactive or interdependent configurations. Networked crowdsourcing allows researchers to capture real-time and transmission-chain interaction between participants to study social cognition and behavior. Here, we use networked crowdsourcing to move the study of real-time interpersonal coordination from the lab and onto Amazon Mechanical Turk, examining how people grow similar over time in their perception and memory."
    
keywords:
    "interpersonal coordination; networked crowdsourcing; human communication; online experiments; social interaction"
    
output: cogsci2016::cogsci_paper
  
---

```{r global_options, include=FALSE}

# clear our workspace
rm(list=ls())

# set global options
require("knitr")
opts_knit$set(root.dir = "../")
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb",
                      fig.path='figs/', echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r import-and-prep-data, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, results='hide'}

# read in the libraries and functions we'll need
source('./supplementary-code/required_packages-pmc.r')
source('./supplementary-code/libraries_and_functions-pmc.r')

# convert the most recent 'perception-memory-coordination.Rmd' to file 
most_recent_pmd = paste0("./supplementary-code/pmc-rmd_to_code-",
                    as.character(as.Date(file.info('perception-memory-coordination.Rmd')$mtime)))
rmd2rscript(infile = './perception-memory-coordination.Rmd',
            outname = most_recent_pmd)

# then source it
source(paste0(most_recent_pmd,'[rmd2r].R'))

# load in participant information
participation_descriptives = read.table('./data/participation_descriptives.csv', sep = ',',
                                        header=TRUE)

```

# Introduction

Research on the phenomenon of *interpersonal coordination* focuses on the subtle ways in which our interactions with others directly affect our own behaviors, feelings, and thoughts. Interest has surged over the last several decades in understanding how contact with others shapes our cognition and behavior. Much of the research on coordination [also known as interactive alignment, interpersonal synchrony, mimicry, and more; see @paxton2016social] focuses on how we become more similar over time, especially during task-oriented or friendly contexts. 

A growing perspective in this area has taken inspiration from dynamical systems theory, conceptualizing interaction as a complex adaptive system from which coordination arises as an emergent phenomenon according to contextual pressures [@riley2011interpersonal]. A fundamental principle of this dynamical systems perspective holds that coordination should not be static across contexts nor over time. Exploring how new contexts and contextual demands---like interpersonal conflict [@paxton2013argument], friendly competition [@tschacher2014nonverbal], or specialized task demands [@ramenzoni2012interpersonal]---has become a central part of this perspective, laying out under what conditions coordination disappears, increases, or demonstrates complementary rather than identical patterns [@fusaroli2012coming]. 

Research is similarly interested in comparing how coordination changes across different behavioral or cognitive systems. Under the dynamical systems perspective, a context's unique pressures, the resulting coordination dynamics, and their impact on the interaction may differ---for example, demonstrating that individuals tend to become more similar over time across a variety of metrics [@louwerse2012behavior] but that specific kinds of coordination can help or hurt outcomes [@fusaroli2012coming] during task-related interaction.

Broadly, during tasks that are neutral [@shockley2003mutual], cooperative [@louwerse2012behavior], or non-confrontationally competitive [e.g., competitive games; @tschacher2014nonverbal], previous work broadly suggests that individuals' behavior and cognition become more similar over time. A range of behavioral signals---both high-level [e.g., gesture; @louwerse2012behavior] and low-level [e.g., postural sway; @shockley2003mutual]---become synchronized, even when the interacting individuals are unable to see one another [@shockley2003mutual] or are separated in time [@richardson2005looking].

The systematic testing of coordination across a variety of interaction contexts is vital to charting its dynamical landscape. This methodical exploration of different factors will eventually enable us to identify control parameters and key factors of initial conditions that shape how coordination emerges and how it impacts interaction outcomes. Doing so, however, requires researchers on interpersonal coordination to expand our view of experimental paradigms: Even as we continue to embrace more complex naturalistic interactions [e.g., @tschacher2014nonverbal], we must continue developing experimental methods for analyzing ``minimally interactive contexts'' [@hale2015using]---that is, situations in which our interactions with others are limited in behavioral channel, scope, or time---to fully map the interaction space.

In the search for both fully interactive and minimally interactive paradigms, we believe that online experiment platforms and crowdsourcing can serve as powerful tools in experiment development and deployment. By connecting people digitally, researchers are able to more fully control the experimental experience, from deciding how much social information about their partner(s) will be available to establishing what communication channel(s) can be utilized to crafting interactive studies for groups beyond the dyad. We propose that researchers interested in social cognition and behavior take advantage of the rapid advances in crowdsourcing platforms to develop real-time experiment models.

## Crowdsourcing for Interactive, Interpersonal Social Experiments

<!-- Recent developments in how data can be collected and analyzed are transforming cognitive science. This is reflected in an increased interest in big data and naturally occurring datasets [@goldstone2016discovering], such as social media activity and video game logs, which hold the promise of capturing behavior in the wild and providing a testing ground for key scientific theories [@paxton2017finding]. While these data can provide a window into observational data about human behavior at a massive scale, technological advances are quickly expanding to accommodate new *experimental* paradigms as well. -->

Crowdsourcing platforms like Amazon Mechanical Turk (http://www.mturk.com) have been extensively used as a means to collect data with relatively simple but robust experimental paradigms, like surveys [@buhrmester2011amazon] and mouse-tracking [@freeman2011hand]. At first, work in this domain required researchers to use survey creation tools or to program bespoke experiments, but more recently, cognitive scientists have worked to create solutions to support the efficient creation of a wider range of experiments [e.g., @gureckis2016psiturk]. 
<!-- As the community around online psychology experiments has grown, it has done so with the intent to broaden its reach (especially to researchers with less programming experience) and to continue to provide more powerful experimental tools. -->

To date, many of these experiment platforms have focused on isolated individuals, making it difficult for researchers to study situated social processes. We believe that the next step in online experimentation, then, is to move to models of real-time social interaction, creating interactive or interdependent experimental paradigms that construct interacting dyads (or even larger networks of people) to understand social processes and phenomena. In doing so, real-time interactive paradigms deployed on crowdsourcing platforms can provide researchers interested in social behavior the opportunity to expand their experimental capabilities beyond the lab while not compromising on the richness and complexity of true interactive contexts.

A recurring concern for using online experiments lies in its participant population. Like all convenience samples, there can be questions about the degree to which the participants reflect the broader population dynamics---including the use of undergraduate students at Western universities as participants in return for course credit, who often do not reflect global demographics [@henrich2010most]. Considerations of sampling and population representativeness are vital for any study, and researchers should carefully consider their sampling choices at the outset of their work. For those interested in using online participants (especially from Amazon Mechanical Turk), recent surveys suggest that U.S.-based MTurk workers are more diverse in a variety of ways than typical college students but not entirely reflective of the general U.S. population [e.g., @buhrmester2011amazon; @paolacci2014inside].

## The Present Study

Here, we build on a robust tradition within interpersonal coordination that demonstrates that people become coordinated across a number of behavioral channels, even when they have very little access to one another. In this case, we focus on task performance within a minimally interactive context through a real-time cooperative online experiment. Specifically, the current study focuses on understanding how interacting individuals become entrained in perception and memory over time, becoming a sort of "line estimation system"---just as two people in the iconic tangram task become a "tangram recognition system" [@dale2011how].

# Method

All research activities were completed in compliance with oversight from Committee for the Protection of Human Subjects at the University of California, Berkeley.

## Participants

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=6, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "Experiment flow."}
img <- png::readPNG("./cogsci2018/figs/workflow_figure.png")
grid::grid.raster(img)
```

Participants (*n* = `r length(participation_descriptives$participant_id)`) were individually recruited from Amazon Mechanical Turk to participate as dyads (*n* = `r length(participation_descriptives$dyad)/2`). Participants were paired with one another according to the order in which they began the experiment. All participants were over 18 years of age and fluent English speakers (self-reported); participation was restricted to only recruit from participants located within the U.S. with a 95\% HIT approval rate.\footnote{A measure of MTurk worker quality, capturing how often their work is rejected by a requester. A 95\% HIT approval rate means that only 5\% of all of their submitted HITs have been rejected.}

The experiment lasted an average of `r round(mean(participation_descriptives$duration),2)` minutes (range: `r round(min(participation_descriptives$duration),2)`---`r round(max(participation_descriptives$duration),2)` minutes). All participants were paid \$1.33 as base pay for finishing the experiment and earned a bonus of up to \$2 for the entire experiment based on mean accuracy over all trials (mean = \$`r sprintf(round(mean(participation_descriptives$bonus-.33),2), fmt = '%#.2f')`; range: \$`r sprintf(round(min(participation_descriptives$bonus-.33),2), fmt = '%#.2f')`---\$`r sprintf(round(max(participation_descriptives$bonus-.33),2), fmt = '%#.2f')`). Participants were not informed about the amount of performance-based bonus that they earned during the experiment.

## Procedure

All data collection procedures were run on Amazon Mechanical Turk (http://mturk.com) using the experiment platform Dallinger (v3.4.1;  http://github.com/dallinger/Dallinger). Code for the experiment is available on GitHub (http://github.com/thomasmorgan/joint-estimation-game), and the resulting experiment data are available on the OSF repository for the project (https://osf.io/8fu7x/).

Each participant was individually recruited on Amazon Mechanical Turk to play a "Line Estimation Memory Game" (advertisement: "Test your memory skills!"). Upon completing informed consent, participants were told that they would be playing a game in which they would be required to remember and re-create line lengths. Participants were informed that they would be complete their training trials individually and would then begin playing with a partner. Participants were given no information about their partner other than the guess that their partner made; no information about the partner's identity was shared.

In each trial, participants were shown 3 red lines, each of a different length (see figure; **NB**: add figure), and were asked to remember all three of them.\footnote{A pilot version of this study showed that participants performed at ceiling when given only 1 line to remember and recreate. The additional 2 lines were added to strictly increase the memory load, as opposed to adding difficulty in other ways (e.g., creating a moving stimulus).} The 3 stimulus lines were displayed for 2 seconds then removed, providing participants with a blank screen for 0.5 seconds. Participants were then told which line to re-create (\#1, \#2, or \#3) and were then given 1 second to submit their guess at how long the target line had been. To do so, participants were given a blank box and used their cursor to fill in the box with a blue line. All lines were presented within bounded boxes of 500 pixels (wide) by 25 pixels (high).

During training, participants were then shown the correct length of the target line (as a grey bar above their own guess) for 2 seconds. This was accompanied by a message telling the participant that they had guessed correctly ("Your guess was correct!") or incorrectly ("Your guess was incorrect") or that they had not submitted a guess within the 1-second time limit ("You didn't respond in time").

During testing, participants' stimulus viewing, waiting, and recreation times remained the same as during testing, but they no longer received information about whether their guess was correct or incorrect. Instead, after both participants had submitted their first guess, participants were shown their guess (in blue) above their partner's guess (in green). Both participants were then asked whether they wanted to change their own guess or to keep the guess they had submitted. If either participant in the dyad indicated that they wanted to change their guess, that participant was then allowed to change their guess (again with a 1-second time limit) *while* still being able to view their partner's guess. Participants who chose to keep their previously submitted guess was informed that their partner chose to submit a new guess and waited for the other participant to finish. At that point, participants were again allowed to change or keep their guess. This process continued until both participants chose to keep their guess.

Participants were informed that their final accuracy would only be calculated for their final guess. However, because they had no means to communicate with their partner about whether each would be accepting or changing their guesses, each participant could not have known whether their decision to keep the guess would have been their final guess for the trial.

For clarity, we will refer to each new stimulus set as a *trial* and to each submitted line length estimate within each trial as a *guess*. This means that some participants may have submitted multiple guesses per trial. The last submitted estimate---the one by which trial-level accuracy is calculated---will be referred to as the *final guess*.

All dyads completed 10 training trials (alone) and 15 test trials (with their partner). All training and test stimuli were randomly generated for each dyad, but both participants within the dyad were given the same stimuli. After participating, each individual participant was asked to complete a series of questionnaires about the game on a series of 1-10 Likert-style scales, including the perceived difficulty of the task, how engaged they were in the task, and questions about their own and their parner's cooperativeness and trustworthiness.

## Measures

### Error 

Error was measured as a ratio relative to the total possible error on a given target stimulus trial. That is, rather than taking a given guess's error relative to the total line length, error was calculated as the maximum *possible* error. For example, if the target stimulus was 60 units long, participants could either under-estimate the line length by 60, or over-estimate it by 40. As such, the maximum possible error for that trial would be 60, and the participant's error would be calculated relative to that maximum possible error.

### Coordination

To measure how participants' perceptual and memory systems became more similar over time, we calculated the cross-correlation coefficient of participants' guess errors across trials [@paxton2013argument], within a window of +/-5 guesses. Although cross-correlation produces information about leading and following behavior, we have no *a priori* expectations about which of the two participants would emerge as a leader (given they have no information about their partner nor any assigned roles), so our first-pass analyses ignore any directionality by averaging across the each incremental lags (i.e., leading/following in both participants' directions).

# Results

All analyses were performed in R [@r2016r]. Linear mixed-effects models were performed using the `lme4` package [@bates2015fitting] using the maximal random slope structure for each random intercept to achieve model convergence [@barr2013random]. All main and interaction terms were centered and standardized prior to entry in the model, allowing the model estimates to be interpretable as effect sizes [@keith2005multiple].

## Model 1: Coordinated Error over Time

```{r build-model-1}

# create model
ccf_main_model = lmer(r ~ lag + training_improvement + 
                        (1 + lag + training_improvement | dyad) +
                        (1 + lag + training_improvement | experiment) +
                        (1 + lag + training_improvement | difficulty),
                      data = ccf_st)

# make it readable
ccf_main_model_readable = xtable_lme(ccf_main_model)

# identify which rows correspond to which value
lag_row = 2
training_row = 3 

```

Our first model tested whether individuals' ratings became more similar over time while accounting for training improvement. We constructed a linear mixed-effects model predicting cross-correlation coefficients of normalized error with absolute lag and training improvement, using maximal random effects structures for experiment and ratings of difficulty after the experiment. As predicted, we find that dyads were significantly and strongly coupled in their error ratings (*\(\beta\)* = `r round(ccf_main_model_readable$Estimate[lag_row],2)`, *p* < `r ccf_main_model_readable$"p-value"[lag_row]+.0001`), with no effect of training improvement (*\(\beta\)* = `r round(ccf_main_model_readable$Estimate[training_row],2)`, *p* < `r ccf_main_model_readable$"p-value"[training_row]`). In other words, we find that players were more likely to produce lines with similar errors at the same time, even across repeated guesses within a single trial, regardless of how well-adapted they were to the task during training.

```{r build-model-2}

# difference the scores between own and partners' error
differences_in_error = info_plot %>%
  
  # start trials at zero and only include tests
  mutate(trial_number = trial_number-10) %>%
  dplyr::filter(trial_type=='test') %>%

  # take the absolute value of normalized error
  mutate(normalized_error = abs(normalized_error)) %>%
  mutate(partner_error = abs(partner_error)) %>%
    
  # create the error from partner variable and the difference between them
  mutate(error_from_partner = abs(normalized_error - partner_error)) %>%
  mutate(both.errors = error_from_partner * normalized_error) %>%
  
  # create error difference variable
  mutate(error_diff = abs(normalized_error - error_from_partner)) %>%
  
  # use response-by-response numbering, not just trial-by-trial
  group_by(experiment, dyad, participant) %>%
  mutate(guess_number = 1:n()) %>%
  ungroup()

# standardize everything
differences_in_error_st = differences_in_error %>%
  mutate_all(funs(as.numeric(scale(as.numeric(.))))) %>%
  mutate_at(vars(participant,dyad),
            funs(factor)) %>%
  select(-trial_type,-network_id) %>%
  na.omit()

# run the model
checking_rates_model = lmer(guess_number ~ normalized_error + error_from_partner + 
                              both.errors + training_improvement + 
                              (1 + both.errors | participant) +
                              (1 + both.errors | difficulty),
                            data = differences_in_error_st)

# make it readable
checking_rates_model_readable = xtable_lme(checking_rates_model)

# identify which rows correspond to which value
normalized_error_row = 2
error_from_partner_row = 3 
both_errors_row = 4
training_row = 5

```

## Model 2: Learning *and* Coordinative Processes

However, during the experiment, both participants are also learning to play the game better over time, in addition to influencing one another. To ensure that the similarity observed in our first model was not simply an artifact of both participants improving individually, we next tested the relation between participants' (1) adaptation to their partner's perceptual estimation and memory and (2) own performance changed over time. If participants were adapting along both avenues, we might find evidence of these dual processes through differences in their rates of adaptation over time. We calculated the former using normalized error ("true error") and the latter as the "error" between the participants' guesses ("partner error").

To answer this question, we built a linear mixed-effects model predicting cumulative guess number with difference between "true error" and "partner error" and participants' training improvement, with the maximal random effects structure for participant and difficulty. As expected, we found that participants' rates of adaptation to their partner significantly differed from their rates of adaptation to the game (*\(\beta\)* = `r round(checking_rates_model_readable$Estimate[error_from_partner_row],2)`, *p* < `r checking_rates_model_readable$"p-value"[error_from_partner_row]`; see Figure 2). In other words, this model suggests that individuals are simultaneously engaging in both learning *and* coordinative processes during the game, becoming attenuated to the learning task while also synchronizing to one another's cognitive processes.

Again, we found no effect of training improvement (*\(\beta\)* = `r round(checking_rates_model_readable$Estimate[training_row],2)`, *p* < `r checking_rates_model_readable$"p-value"[training_row]`).

```{r , fig.env = "figure", fig.pos = "H", fig.align='left', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Difference over time in coordinative and learning processes, or the change in guess deviation from truth (in blue) and from their partner's guess (in red) over the game."}

# create a plot to show difference in slopes
partner_vs_truth_slope_plot = ggplot(differences_in_error,
                                     aes(x = trial_number,
                                         y = normalized_error)) +
  
  # smooth the from-truth error
  geom_smooth(method='lm',
              formula=y~x,
              aes(colour='From truth')) +
  
  # smooth the from-partner error
  geom_smooth(aes(x = trial_number,
                  y = error_from_partner,
                  colour = 'From partner'),
              method = 'lm',
              formula=y~x) +
  
  # label
  ylab('Normalized absolute\nerror of guess') +
  scale_x_continuous(breaks=c(1,5,10,15)) +
  xlab('Trial') +
  ggtitle('Participant guess error from truth\nand from partner over test trials') +
  theme(legend.position="bottom",
        legend.direction="horizontal",
        axis.title = element_text(size = 10),
        text = element_text(size = 10),
        plot.title = element_text(size = 10),
        legend.text=element_text(size=6),
        legend.title=element_text(size=7)) +
  scale_colour_discrete(name="Error source")

# save a high-resolution version of the plot
ggsave(plot = partner_vs_truth_slope_plot,
       height = 3,
       width = 5,
       filename = './figures/pmc-partner_vs_truth_slope_plot.jpg')

# save a smaller version of the plot for knitr
ggsave(plot = partner_vs_truth_slope_plot,
       height = 3,
       width = 5,
       dpi=100,
       filename = './figures/pmc-partner_vs_truth_slope_plot-knitr.jpg')

# plot
partner_vs_truth_slope_plot

```

## Model 3: Social Factors in Minimally Interacting Contexts

```{r build-model-3}

# get the maximum number of guess changes at each trial
social_parameters = differences_in_error %>% ungroup() %>%
  group_by(experiment, dyad, participant, trial_number) %>%
  dplyr::filter(guess_counter == max(guess_counter) |
                  guess_counter == min(guess_counter)) %>%
  ungroup()

# get the original guess and the last guess
first_guess = social_parameters %>%
  group_by(experiment, dyad, participant, trial_number) %>%
  dplyr::filter(guess_counter==min(guess_counter)) %>%
  ungroup() %>%
  select(experiment, dyad, participant, trial_number, error_diff) %>%
  rename('first_error_diff' = 'error_diff')

# merge into the original
social_parameters = social_parameters %>% ungroup() %>%
  group_by(experiment, dyad, participant, trial_number) %>%
  dplyr::filter(guess_counter==max(guess_counter)) %>%
  ungroup() %>%
  rename('final_error_diff' = 'error_diff') %>%
  left_join(., first_guess,
            by=c('experiment',
                 'dyad',
                 'participant',
                 'trial_number')) %>%
  
  # grab only one row
  distinct()

# standardize everything
social_parameters_st = social_parameters %>% ungroup() %>%
  mutate_all(funs(as.numeric(scale(as.numeric(.))))) %>%
  select(-trial_type,-network_id)

# build the model
trust_model = lmer(guess_counter ~ trust_partner + trial_number + first_error_diff + training_improvement +
                     (1 + trust_partner + trial_number | participant) +
                     (1 + trust_partner + trial_number | difficulty),
                   data = social_parameters_st)
trust_model_model_readable = xtable_lme(trust_model)

# get row info
trust_partner_row = 2
trial_number_row = 3
first_error_row = 4
training_row = 5

```

To explore the role that social judgements can play even in minimally interactive contexts, we finally turned to consider how trust might impact a participant's willingness to change their guess. We created a linear-mixed effects model predicting the total number of times a participant changed their guess during a given trial according to their trust rating of their partner and the difference in participants' first guess on that trial, while controlling for trial number and how much they improved during their own training. Again, the model included the maximal random effects structure for participant and difficulty. 

We found that greater trust in their partner predicted an *increase* in the number of iterations of guesses within a trial (*\(\beta\)* = `r round(trust_model_model_readable$Estimate[trust_partner_row],2)`, *p* < `r trust_model_model_readable$"p-value"[trust_partner_row]`).  Interestingly, we found an effect of training improvement (*\(\beta\)* = `r round(checking_rates_model_readable$Estimate[training_row],2)`, *p* < `r checking_rates_model_readable$"p-value"[training_row]`), showing that people take more guesses on each trial when they did better in their training trials. We also saw an effect of trial number (*\(\beta\)* = `r round(checking_rates_model_readable$Estimate[trial_number_row],2)`, *p* < `r checking_rates_model_readable$"p-value"[trial_number_row]`), indicating that people changed their guesses fewer times as the game wore on.

# Discussion

One plausible explanation for this behavior may be that participants were more willing to change their own guess to be closer to the guess provided by a partner whose perceptual and memory capabilites they trusted. Alternatively, given that the measure of trust was taken at the end of the experiment, there may have been a

## Future Directions

Injecting social dynamics in experimentally and/or as much as is desired

# Conclusion

# Acknowledgements

Our thanks go to Nelle Varoquaux and to the Dallinger development team for their assistance in debugging the experiment.  

This work was funded in part by DARPA Cooperative Agreement D17AC00004.

# References 

```{r references}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent


<!-- # Formalities, Footnotes, and Floats -->

<!-- Use standard APA citation format. Citations within the text should -->
<!-- include the author's last name and year. If the authors' names are -->
<!-- included in the sentence, place only the year in parentheses, as in -->
<!-- [-@NewellSimon1972a], but otherwise place the entire reference in -->
<!-- parentheses with the authors and year separated by a comma -->
<!-- [@NewellSimon1972a]. List multiple references alphabetically and -->
<!-- separate them by semicolons [@ChalnickBillman1988a; @NewellSimon1972a].  -->
<!-- Use the et. al. construction only after listing all the authors to a -->
<!-- publication in an earlier reference and for citations with four or -->
<!-- more authors. -->

<!-- For more information on citations in R Markdown, see **[here](http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html#citations).** -->

<!-- ## Footnotes -->

<!-- Indicate footnotes with a number\footnote{Sample of the first -->
<!-- footnote.} in the text. Place the footnotes in 9 point type at the -->
<!-- bottom of the page on which they appear. Precede the footnote with a -->
<!-- horizontal rule.\footnote{Sample of the second footnote.} -->

<!-- ## Figures -->

<!-- All artwork must be very dark for purposes of reproduction and should -->
<!-- not be hand drawn. Number figures sequentially, placing the figure -->
<!-- number and caption, in 10 point, after the figure with one line space -->
<!-- above the caption and one line space below it. If necessary, leave extra white space at -->
<!-- the bottom of the page to avoid splitting the figure and figure -->
<!-- caption. You may float figures to the top or bottom of a column, or -->
<!-- set wide figures across both columns. -->

<!-- ## Two-column images -->

<!-- You can read local images using png package for example and plot  -->
<!-- it like a regular plot using grid.raster from the grid package.  -->
<!-- With this method you have full control of the size of your image. **Note: Image must be in .png file format for the readPNG function to work.** -->

<!-- You might want to display a wide figure across both columns. To do this, you change the `fig.env` chunk option to `figure*`. To align the image in the center of the page, set `fig.align` option to `center`. To format the width of your caption text, you set the `num.cols.cap` option to `2`. -->

<!-- ```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."} -->

<!-- ``` -->

<!-- ## One-column images -->

<!-- Single column is the default option, but if you want set it explicitly, set `fig.env` to `figure`. Notice that the `num.cols` option for the caption width is set to `1`. -->

<!-- ```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "One column image."} -->

<!-- ``` -->


<!-- ## R Plots -->

<!-- You can use R chunks directly to plot graphs. And you can use latex floats in the -->
<!-- fig.pos chunk option to have more control over the location of your plot on the page. For more information on latex placement specifiers see **[here](https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions)** -->

<!-- ```{r plot, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=2, fig.height=2, fig.cap = "R plot" } -->
<!-- x <- 0:100 -->
<!-- y <- 2 * (x + rnorm(length(x), sd = 3) + 3) -->

<!-- ggplot2::ggplot(data = data.frame(x, y),  -->
<!--        aes(x = x, y = y)) +  -->
<!--   geom_point() +  -->
<!--   geom_smooth(method = "lm") -->
<!-- ``` -->


<!-- ## Tables -->

<!-- Number tables consecutively; place the table number and title (in -->
<!-- 10 point) above the table with one line space above the caption and -->
<!-- one line space below it, as in Table 1. You may float -->
<!-- tables to the top or bottom of a column, set wide tables across both -->
<!-- columns. -->

<!-- You can use the xtable function in the xtable package. -->

<!-- ```{r xtable, results="asis"} -->
<!-- n <- 100 -->
<!-- x <- rnorm(n) -->
<!-- y <- 2*x + rnorm(n) -->
<!-- out <- lm(y ~ x) -->

<!-- tab1 <- xtable::xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2),  -->
<!--                       caption = "This table prints across one column.") -->

<!-- print(tab1, type="latex", comment = F, table.placement = "H") -->
<!-- ``` -->
