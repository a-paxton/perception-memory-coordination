---
title: "Interpersonal Coordination in Perception and Memory in an Online Experiment: Using Dallinger for Experiments on Interaction"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: > 
    \author{{\large \bf Alexandra Paxton} \\ \texttt{paxton.alexandra@gmail.com} \\ Institute of Cognitive and Brain Sciences \\ Berkeley Institute for Data Science \\ University of California, Berkeley
    \And {\large \bf Thomas J. H. Morgan} \\ \texttt{thomas.j.h.morgan@asu.edu} \\ School of Human Evolution and Social Change \\ Arizona State University
    \AND {\large \bf Jordan W. Suchow} \\ \texttt{suchow@berkeley.edu} \\ Social Science Matrix \\ University of California, Berkeley
    \And {\large \bf Thomas L. Griffiths} \\ \texttt{tom\_griffiths@berkeley.edu} \\ Department of Psychology \\ University of California, Berkeley}
    
abstract: 
    "With cognitive scientists' increasing interest in moving outside of the lab, recent advances in crowdsourcing platforms can help strike a balance between the tight experimental control of lab designs and the affordances of web-based experiments to reach beyond traditional undergraduate subject pools. By taking advantage of new tools, scientists interested in social cognition and behavior can create new designs and adapt traditional ones to deliver experiments at scale. Dallinger is one such tool, providing researchers with an open-source experiment platform that provides end-to-end automation of the experiment pipeline, from participant recruitment and consent, to data de-identification and participant compensation. Here we demonstrate how Dallinger can be used to run complex experimental studies of interactive human social behavior, as a demonstration of its potential to study social cognition and behavior using designs drawn from across cognitive science."
    
keywords:
    "interpersonal interaction; human communication; crowdsourcing; Dallinger"
    
output: cogsci2016::cogsci_paper
  
---

```{r global_options, include=FALSE}

# clear our workspace
rm(list=ls())

# set global options
require("knitr")
opts_knit$set(root.dir = "../")
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb",
                      fig.path='figs/', echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r import-and-prep-data, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, results='hide'}

# read in the libraries and functions we'll need
source('./supplementary-code/required_packages-pmc.r')
source('./supplementary-code/libraries_and_functions-pmc.r')

# convert the most recent 'perception-memory-coordination.Rmd' to file 
most_recent_pmd = paste0("./supplementary-code/pmc-rmd_to_code-",
                    as.character(as.Date(file.info('perception-memory-coordination.Rmd')$mtime)))
rmd2rscript(infile = './perception-memory-coordination.Rmd',
            outname = most_recent_pmd)

# then source it
source(paste0(most_recent_pmd,'[rmd2r].R'))

# load in participant information
participant_counts = read.table('./data/participant_pairs.csv', sep = ',',
                                header=TRUE)
participation_descriptives = read.table('./data/partipation_descriptives.csv', sep = ',',
                                        header=TRUE)

```

# Introduction

Recent developments in how data can be collected and analyzed are transforming cognitive science. This is reflected in an increased interest in big data and naturally occurring datasets [@goldstone2016discovering], such as social media activity and video game logs, which hold the promise of capturing behavior in the wild and providing a testing ground for key scientific theories [@paxton2017finding]. While these data can provide a window into observational data about human behavior at a massive scale, technological advances are quickly expanding to accommodate new *experimental* paradigms as well.

Crowdsourcing platforms like Amazon Mechanical Turk (http://www.mturk.com) have been extensively used as a means to collect data with relatively simple but robust experimental paradigms, like surveys [@buhrmester2011amazon] and mouse-tracking [@freeman2011hand]. At first, work in this domain required researchers to use established survey creation tools, which were quick to do but constrained experimental designs, or to program bespoke experiments, which is more open ended but far more time-consuming. More recently, cognitive scientists have worked to create solutions to support the efficient creation of a wider range of experiments [e.g., @gureckis2016psiturk]. As the community around online psychology experiments has grown, it has done so with the intent to broaden its reach (especially to researchers with less programming experience) and to continue to provide more powerful experimental tools.

Dallinger is another step forward in this endeavor: We believe that it can provide researchers interested in social behavior the opportunity to expand their experimental capabilities beyond the lab while not compromising on the richness and complexity of true interactive contexts.

## Interpersonal Coordination

We here focus on the phenomenon of *interpersonal coordination*, an interdisciplinary research area that focuses on the ways in which individuals affect one another over time as a result of their interaction [also known as interactive alignment, interpersonal synchrony, mimicry, and more; see @paxton2016social]. This area is increasingly marked by principles of dynamical systems theory, conceptualizing interaction as a complex adaptive system [@riley2011interpersonal]. A fundamental principle of this is that the emergent behavior---in this case, interpersonal coordination---is not static but changes over time.

## Dallinger

Like many other experimental platforms [e.g., psiTurk; @gureckis2016psiturk], Dallinger allows scientists to design experiments and deploy them online where participants can be recruited through crowdsourcing. While the experimenter must create the experimental interface, the bulk of the rest of the experiment (e.g., recruitment, data collection, participant recruitment, base and bonus payment) is handled by prebuilt functions included in Dallinger. 

What marks Dallinger as different from the variety of otherwise similar platforms is its focus on *networks*. Instead of handling participants as individuals, Dallinger is specialized to automatically arrange participants into a variety of network structures, connecting them with numerous functions to manage communication among the participants. As such, Dallinger is uniquely positioned to support experimental research into human social behavior at scale.

Dallinger relies on Amazon Mechanical Turk for participant recruitment, and a recurring concern for using online experiments lies in its participant population. Like all convenience samples, there can be questions about the degree to which the participants reflect the broader population dynamics---including the use of undergraduate students at Western universities as participants in return for course credit, who often do not reflect global demographics [@henrich2010most]. Considerations of sampling and population representativeness are vital for any study, and researchers should carefully consider their sampling choices at the outset of their work. For those interested in using online participants (especially from Amazon Mechanical Turk), recent surveys suggest that U.S.-based MTurk workers are more diverse in a variety of ways than typical college students but not entirely reflective of the general U.S. population [e.g., @buhrmester2011amazon; @paolacci2014inside].

## The Present Study

The current study focuses on understanding how minimally interacting individuals become entrained in perception and memory over time, becoming a sort of "line estimation system"---just as two people in an lab experiment become a "tangram recognition system" [@dale2011how]. 

To illustrate the utility of Dallinger for research into social behaviors, w

# Method

All research activities were completed in compliance with oversight from Committee for the Protection of Human Subjects at the University of California, Berkeley.

## Participants

Participants (*n* = `r length(participant_counts$dyad)*2`) were individually recruited from Amazon Mechanical Turk to participate as dyads (*n* = `r length(participant_counts$dyad)`). Participants were paired with one another according to the order in which they began the experiment. All participants were over 18 years of age and fluent English speakers (self-reported); participation was restricted to only recruit from participants located within the U.S. with a 95% HIT approval rate\footnote{A measure of MTurk worker quality, capturing how often their work is rejected by a requester.}.

<!-- The experiment lasted an average of `r round(mean(participation_descriptives$duration),2)` minutes (range: `r round(min(participation_descriptives$duration),2)`---`r round(max(participation_descriptives$duration),2)` minutes). In return for their participation, all participants were paid \$1.33 as base pay for finishing the experiment. Each participant also earned a bonus based of up to \$2 for the entire experiment based on mean accuracy over all trials (mean = \$`r sprintf(round(mean(participation_descriptives$bonus-.33),2), fmt = '%#.2f')`; range: \$`r sprintf(round(min(participation_descriptives$bonus-.33),2), fmt = '%#.2f')`---\$`r sprintf(round(max(participation_descriptives$bonus-.33),2), fmt = '%#.2f')`). -->

## Procedure

All data collection procedures were completed through the experiment platform Dallinger (v3.4.1;  http://github.com/dallinger/Dallinger), deployed on Amazon Mechanical Turk (http://mturk.com). Code for the experiment is available on GitHub (http://github.com/thomasmorgan/joint-estimation-game), and the resulting experiment data are available on the OSF repository for the project (https://osf.io/8fu7x/).

Each participant was individually recruited on Amazon Mechanical Turk to play a "Line Estimation Memory Game" (advertisement: "Test your memory skills!"). Upon completing informed consent, participants were told that they would be playing a game in which they would be required to remember and re-create line lengths. Participants were informed that they would be complete their training trials individually and would then begin playing with a partner. Participants were given no information about their partner other than the guess that their partner made; no information about the partner's identity was shared.

In each trial, participants were shown 3 red lines, each of a different length (see figure; **NB**: add figure), and were asked to remember all three of them.\footnote{A pilot version of this study showed that participants performed at ceiling when given only 1 line to remember and recreate. The additional 2 lines were added to strictly increase the memory load, as opposed to adding difficulty in other ways (e.g., creating a moving stimulus).} The 3 stimulus lines were displayed for 2 seconds then removed, providing participants with a blank screen for 0.5 seconds. Participants were then told which line to re-create (\#1, \#2, or \#3) and were then given 1 second to submit their guess at how long the target line had been. To do so, participants were given a blank box and used their cursor to fill in the box with a blue line. All lines were presented within bounded boxes of 500 pixels (wide) by 25 pixels (high).

During training, participants were then shown the correct length of the target line (as a grey bar above their own guess) for 2 seconds. This was accompanied by a message telling the participant that they had guessed correctly ("Your guess was correct!") or incorrectly ("Your guess was incorrect") or that they had not submitted a guess within the 1-second time limit ("You didn't respond in time").

During testing, participants' stimulus viewing, waiting, and recreation times remained the same as during testing, but they no longer received information about whether their guess was correct or incorrect. Instead, after both participants had submitted their first guess, participants were shown their guess (in blue) above their partner's guess (in green). Both participants were then asked whether they wanted to change their own guess or to keep the guess they had submitted. If either participant in the dyad indicated that they wanted to change their guess, that participant was then allowed to change their guess (again with a 1-second time limit) *while* still being able to view their partner's guess. Participants who chose to keep their previously submitted guess was informed that their partner chose to submit a new guess and waited for the other participant to finish. At that point, participants were again allowed to change or keep their guess. This process continued until both participants chose to keep their guess.

Participants were informed that their final accuracy would only be calculated for their final guess. However, because they had no means to communicate with their partner about whether each would be accepting or changing their guesses, each participant could not have known whether their decision to keep the guess would have been their final guess for the trial.

For clarity, we will refer to each new stimulus set as a *trial* and to each submitted line length estimate within each trial as a *guess*. This means that some participants may have submitted multiple guesses per trial. The last submitted estimate---the one by which trial-level accuracy is calculated---will be referred to as the *final guess*.

All dyads completed 10 training trials (alone) and 15 test trials (with their partner). All training and test stimuli were randomly generated for each dyad, but both participants within the dyad were given the same stimuli. After participating, each individual participant was asked to complete a series of questionnaires about the game on a series of 1-10 Likert-style scales, including the perceived difficulty of the task, how engaged they were in the task, and questions about their own and their parner's cooperativeness and trustworthiness.

## Analyses

### Similarity

To measure how participants' perceptual and memory systems became more similar over time, we calculated the cross-correlation coefficient of participants' guess errors across trials [@paxton2013argument], within a window of +/-5 guesses. Although cross-correlation produces information about leading and following behavior, we have no *a priori* expectations about which of the two participants would emerge as a leader (given they have no information about their partner nor any assigned roles), so our first-pass analyses ignore any directionality by averaging across the each incremental lags (i.e., leading/following in both participants' directions).

### Accuracy 

Accuracy was measured as a ratio relative to the total possible error on a given target stimulus trial. That is, rather than taking a given guess's error relative to the total line length, error was calculated as the maximum *possible* error. For example, if the target stimulus was 60 units long, the maximum possible error for that trial would be 60, and the participant's error would be calculated relative to that maximum possible error.

# Results

All analyses were performed in R [@r2016r]. Linear mixed-effects models were performed using the `lme4` package [@bates2015fitting] using the maximal random slope structure for each random intercept to achieve model convergence [@barr2013random].

```{r model-1}

```

# Discussion

# Conclusion

# Acknowledgements

Thanks go to the Dallinger development team for their assistance in executing the experiment.

This work was funded in part by the **DALLINGER GRANT INFO GOES HERE**. 

# References 

```{r references}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent


<!-- # Formalities, Footnotes, and Floats -->

<!-- Use standard APA citation format. Citations within the text should -->
<!-- include the author's last name and year. If the authors' names are -->
<!-- included in the sentence, place only the year in parentheses, as in -->
<!-- [-@NewellSimon1972a], but otherwise place the entire reference in -->
<!-- parentheses with the authors and year separated by a comma -->
<!-- [@NewellSimon1972a]. List multiple references alphabetically and -->
<!-- separate them by semicolons [@ChalnickBillman1988a; @NewellSimon1972a].  -->
<!-- Use the et. al. construction only after listing all the authors to a -->
<!-- publication in an earlier reference and for citations with four or -->
<!-- more authors. -->

<!-- For more information on citations in R Markdown, see **[here](http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html#citations).** -->

<!-- ## Footnotes -->

<!-- Indicate footnotes with a number\footnote{Sample of the first -->
<!-- footnote.} in the text. Place the footnotes in 9 point type at the -->
<!-- bottom of the page on which they appear. Precede the footnote with a -->
<!-- horizontal rule.\footnote{Sample of the second footnote.} -->

<!-- ## Figures -->

<!-- All artwork must be very dark for purposes of reproduction and should -->
<!-- not be hand drawn. Number figures sequentially, placing the figure -->
<!-- number and caption, in 10 point, after the figure with one line space -->
<!-- above the caption and one line space below it. If necessary, leave extra white space at -->
<!-- the bottom of the page to avoid splitting the figure and figure -->
<!-- caption. You may float figures to the top or bottom of a column, or -->
<!-- set wide figures across both columns. -->

<!-- ## Two-column images -->

<!-- You can read local images using png package for example and plot  -->
<!-- it like a regular plot using grid.raster from the grid package.  -->
<!-- With this method you have full control of the size of your image. **Note: Image must be in .png file format for the readPNG function to work.** -->

<!-- You might want to display a wide figure across both columns. To do this, you change the `fig.env` chunk option to `figure*`. To align the image in the center of the page, set `fig.align` option to `center`. To format the width of your caption text, you set the `num.cols.cap` option to `2`. -->

<!-- ```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=4, fig.height=2, fig.align = "center", set.cap.width=T, num.cols.cap=2, fig.cap = "This image spans both columns. And the caption text is limited to 0.8 of the width of the document."} -->

<!-- ``` -->

<!-- ## One-column images -->

<!-- Single column is the default option, but if you want set it explicitly, set `fig.env` to `figure`. Notice that the `num.cols` option for the caption width is set to `1`. -->

<!-- ```{r image, fig.env = "figure", fig.pos = "H", fig.align='center', fig.width=2, fig.height=2, set.cap.width=T, num.cols.cap=1, fig.cap = "One column image."} -->

<!-- ``` -->


<!-- ## R Plots -->

<!-- You can use R chunks directly to plot graphs. And you can use latex floats in the -->
<!-- fig.pos chunk option to have more control over the location of your plot on the page. For more information on latex placement specifiers see **[here](https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions)** -->

<!-- ```{r plot, fig.env="figure", fig.pos = "H", fig.align = "center", fig.width=2, fig.height=2, fig.cap = "R plot" } -->
<!-- x <- 0:100 -->
<!-- y <- 2 * (x + rnorm(length(x), sd = 3) + 3) -->

<!-- ggplot2::ggplot(data = data.frame(x, y),  -->
<!--        aes(x = x, y = y)) +  -->
<!--   geom_point() +  -->
<!--   geom_smooth(method = "lm") -->
<!-- ``` -->


<!-- ## Tables -->

<!-- Number tables consecutively; place the table number and title (in -->
<!-- 10 point) above the table with one line space above the caption and -->
<!-- one line space below it, as in Table 1. You may float -->
<!-- tables to the top or bottom of a column, set wide tables across both -->
<!-- columns. -->

<!-- You can use the xtable function in the xtable package. -->

<!-- ```{r xtable, results="asis"} -->
<!-- n <- 100 -->
<!-- x <- rnorm(n) -->
<!-- y <- 2*x + rnorm(n) -->
<!-- out <- lm(y ~ x) -->

<!-- tab1 <- xtable::xtable(summary(out)$coef, digits=c(0, 2, 2, 1, 2),  -->
<!--                       caption = "This table prints across one column.") -->

<!-- print(tab1, type="latex", comment = F, table.placement = "H") -->
<!-- ``` -->
