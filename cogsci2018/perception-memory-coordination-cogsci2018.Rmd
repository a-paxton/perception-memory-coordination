---
title: "Interpersonal Coordination of Perception and Memory in Real-Time Online Social Experiments"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: >
    \author{{\large \bf Alexandra Paxton} \\ \texttt{paxton.alexandra@gmail.com} \\ Institute of Cognitive and Brain Sciences \\ Berkeley Institute for Data Science \\ University of California, Berkeley
    \And {\large \bf Thomas J. H. Morgan} \\ \texttt{thomas.j.h.morgan@asu.edu} \\ School of Human Evolution and Social Change \\ Arizona State University
    \AND {\large \bf Jordan W. Suchow} \\ \texttt{suchow@berkeley.edu} \\ Social Science Matrix \\ University of California, Berkeley
    \And {\large \bf Thomas L. Griffiths} \\ \texttt{tom\_griffiths@berkeley.edu} \\ Department of Psychology \\ University of California, Berkeley}
    
abstract: 

abstract:
    "The quiet hum of interpersonal coordination that runs throughout social communication and interaction shows how individuals can subtly influence one another's behaviors, thoughts, and emotions over time. While the majority of research on coordination studies face-to-face interaction, recent advances in crowdsourcing afford the opportunity to conduct large-scale, real-time social interaction experiments. We take advantage of these tools to explore interpersonal coordination in a ``minimally interactive context,'' distilling the richness of natural communication into a tightly controlled setting to explore how people become coupled in their perceptual and memory systems while performing a task together. Consistent with previous work on postural sway and gaze, we find that individuals become coupled to one another's cognitive processes without needing to be co-located or fully interactive with their partner; interestingly, although participants had neither information about nor direct communication with their partner, we also find hints of the context-shaping effects that social forces can play in face-to-face interaction."

keywords:
    "interpersonal coordination; human communication; online experiments; social interaction"
    
output: cogsci2016::cogsci_paper
  
---

```{r global_options, include=FALSE}

# clear our workspace
rm(list=ls())

# set global options
require("knitr")
opts_knit$set(root.dir = "../")
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb",
                      fig.path='figs/', echo=F, warning=F, cache=F, 
                      message=F, sanitize = T)
```

```{r import-and-prep-data, echo=FALSE, warning=FALSE, message=FALSE, error=FALSE, results='hide'}

# read in the libraries and functions we'll need
source('./supplementary-code/required_packages-pmc.r')
source('./supplementary-code/libraries_and_functions-pmc.r')

# convert the most recent 'perception-memory-coordination.Rmd' to file 
most_recent_pmd = paste0("./supplementary-code/pmc-rmd_to_code-",
                    as.character(as.Date(file.info('perception-memory-coordination.Rmd')$mtime)))
rmd2rscript(infile = './perception-memory-coordination.Rmd',
            outname = most_recent_pmd)

# then source it
source(paste0(most_recent_pmd,'[rmd2r].R'))

# load in participant information
participation_descriptives = read.table('./data/participation_descriptives.csv', sep = ',',
                                        header=TRUE)

```

# Introduction

Research on the phenomenon of *interpersonal coordination* focuses on the subtle ways in which our interactions with others directly affect our own behaviors, feelings, and thoughts. Interest in coordination [also known as interactive alignment, interpersonal synchrony, mimicry, and more; see @paxton2016social] has surged over the last several decades as a framework for understanding how contact with others shapes our cognition and behavior, with much of it focusing on how we become more similar over time in task-oriented or friendly contexts.

A growing perspective in this area has taken inspiration from dynamical systems theory, conceptualizing interaction as a complex adaptive system from which coordination arises as an emergent phenomenon according to contextual pressures [@riley2011interpersonal]. A fundamental principle of this dynamical systems perspective holds that coordination should not be static across contexts nor over time. Exploring new contexts and contextual demands---like interpersonal conflict [@paxton2013argument], friendly competition [@tschacher2014nonverbal], or specialized task demands [@fusaroli2012coming]---has become a central part of this perspective, laying out under what conditions coordination disappears, increases, or demonstrates complementary rather than synchronous in-phase patterns.

There is similar interest in comparing how coordination changes across different behavioral or cognitive systems. Under the dynamical systems perspective, a context's unique pressures, the resulting coordination dynamics, and their impact on the interaction may differ over time and across settings. For example, some of the earliest work in this subset of coordination research has found that---during task-related interaction---individuals tend to become more similar over time across a variety of metrics [@louwerse2012behavior] but that specific kinds of coordination can differentially help or hurt outcomes [@fusaroli2012coming].

Broadly, during tasks that are neutral [@shockley2003mutual], cooperative [@louwerse2012behavior], or competitive [but not conflict-driven; e.g., competitive games, @tschacher2014nonverbal], individuals' behavior and cognition become more similar over time. A range of behavioral signals, both high-level [e.g., gesture; @louwerse2012behavior] and low-level [e.g., postural sway; @shockley2003mutual], become synchronized during interaction. This synchronization occurs even when the interacting individuals are unable to see one another [@shockley2003mutual] or are separated in time [@richardson2005looking].

The systematic testing of coordination across a variety of interaction contexts is vital to charting its dynamical landscape. This methodical exploration of different factors will eventually enable us to identify control parameters and key factors of initial conditions that shape how coordination emerges and how it impacts interaction outcomes. Doing so, however, requires an expanded view of experimental paradigms: Even as we continue to embrace more complex naturalistic interactions [e.g., @paxton2013argument; @tschacher2014nonverbal], to fully map the interaction space we must also develop experimental methods for analyzing "minimally interactive contexts" [@hale2015using]---that is, situations in which our interactions with others are limited in behavioral channel, scope, or time.

Online experiment platforms and crowdsourcing can be powerful tools for creating both fully interactive and minimally interactive paradigms. By connecting people digitally, researchers can fully control the experimental experience---from deciding how much social information partners will have about one another, to establishing which communication channels can be used, to crafting interactive studies for groups beyond the dyad. Crowdsourcing platforms such as Amazon Mechanical Turk (http://www.mturk.com) have been extensively used as a means to collect data on individuals [@buhrmester2011amazon]. However, by developing real-time interactive paradigms for these platforms, researchers interested in social behavior can study experimentally situated social processes beyond the lab without compromising the richness and complexity of true interactive contexts.

## The Present Study

Here, we build on the finding that people become coordinated across multiple behavioral channels even when they have very little access to each other. We focus on task performance within a minimally interactive context through a real-time cooperative online experiment---a nominal game that asks players to correctly perceive and remember the position and size of a line, while under cognitive load. Specifically, the current study focuses on understanding how interacting individuals become entrained in perception and memory over time, becoming a sort of "line estimation system" [cf. @dale2011how].

We approach the current study with three main research questions. First, we ask whether people become more coupled in their perceptual and memory systems over time, despite limited perceptual and social information about their partner. Next, we investigate whether any observed coordination effects could simply be an artifact of the joint learning context. Finally, we look to whether any social factors (such as rapport and affect, which play vital roles in face-to-face interaction [e.g., @tschacher2014nonverbal]) might influence these dynamics, despite the minimal context. We are interested to explore whether some social influences surface as emergent effects even though the game does not facilitate any explicit social behaviors.

While a relatively simple paradigm, this research program contributes to the theoretical landscape of interpersonal coordination research in several ways. First, the investigation continues to expand work on minimally interactive contexts---an underexplored avenue in interpersonal coordination research, which often relies on fully interactive paradigms. This work, then, expands coordination research to explore whether coordination occurs even when interacting individuals have access only to one impoverished channel of information available to them. Second, we explore the degree to even low-level properties of memory and perception become entrained, despite this minimal information---explicitly probing questions of memory systems posed by transactive memory [@tollefsen2013alignment]. Finally, we extend complex investigations of social impacts on coordination into lower-level behavioral and perceptual channels. Taken together, the present study aims to test several hypotheses that are circumscribed by a host of related previous work, providing explicit tests for ideas that are often implicitly accepted by coordination researchers.

# Method

All research activities were completed in compliance with oversight from the Committee for the Protection of Human Subjects at the University of California, Berkeley.

## Participants

```{r 2-col-image, fig.env = "figure*", fig.pos = "h", fig.width=7, fig.align = "center", set.cap.width=F, num.cols.cap=2, fig.cap = "Experiment flow"}
img <- png::readPNG("./cogsci2018/figs/workflow_figure.png")
grid::grid.raster(img)
```

Participants (*n* = `r length(participation_descriptives$participant_id)`) were individually recruited from Amazon Mechanical Turk to participate as dyads (*n* = `r length(participation_descriptives$dyad)/2`). Participants were paired in the order they arrived to experiment. All participants were over 18 years of age and were fluent English speakers (self-reported); recruitment was restricted to participants located within the U.S. with a 95\% approval rate for their HITs (*Human Intelligence Tasks*; i.e., tasks posted on MTurk).\footnote{HIT approval rate is a measure of MTurk worker quality, capturing how often their work is rejected by a requester. A 95\% HIT approval rate means that only 5\% of all of their submitted HITs have been rejected.}

The experiment lasted an average of `r round(mean(participation_descriptives$duration),2)` minutes (range: `r round(min(participation_descriptives$duration),2)`---`r round(max(participation_descriptives$duration),2)` minutes). All participants were paid \$1.33 as base pay for finishing the experiment and earned a bonus of up to \$2 for the entire experiment based on their own mean accuracy over all trials (mean = \$`r sprintf(round(mean(participation_descriptives$bonus-.33),2), fmt = '%#.2f')`; range: \$`r sprintf(round(min(participation_descriptives$bonus-.33),2), fmt = '%#.2f')`---\$`r sprintf(round(max(participation_descriptives$bonus-.33),2), fmt = '%#.2f')`). Participants were not aware of the value of their earned bonus until after completing the experiment.

## Procedure

Data collection was run on Amazon Mechanical Turk (http://mturk.com) using the experiment platform Dallinger (v3.4.1;  http://github.com/dallinger/Dallinger). Code for the experiment is available on GitHub (http://github.com/thomasmorgan/joint-estimation-game).

Each participant was individually recruited on Amazon Mechanical Turk to play a "Line Estimation Memory Game" (advertisement: "Test your memory skills!"; see Fig. 1 for experiment flow). Upon completing informed consent, participants were told that they would be playing a game in which they would be required to remember and recreate line lengths. Participants were told that they would first complete their training trials individually and then begin to play with a partner. Participants were given no information about their partner other than the guess that their partner made; no information about the partner's identity was shared. Participants were told in advance that they would receive a bonus for their own final guess's accuracy in the test trials.

On each trial (i.e., each new stimulus set), participants were shown 3 red lines, each of a different length, and were asked to remember all three.\footnote{In a pilot study, participants performed at ceiling when given only 1 line to remember and recreate. Two more lines were added to increase the memory load.} The three  lines were displayed for 2 s then replaced with a blank screen for 0.5 s. Participants were then told which line to re-create (\#1, \#2, or \#3) and were then given 1 s to submit their guess at how long the target line had been. To do so, participants were given a blank box and used their cursor to fill in the box with a blue line. All lines were presented within bounded 500-by-25-pixel boxes.

During training, participants were given feedback in the form of the true length of the target line (as a grey bar above their own guess) for 2 s. This was accompanied by a message telling the participant that they had guessed correctly ("Your guess was correct!"), incorrectly ("Your guess was incorrect"), or that they had not submitted a guess within the 1-s time limit ("You didn't respond in time"). During training, a guess was "correct" if it was within 4 pixels of the true line length; however, accuracy is modeled as a continuous value.

During testing, participants no longer received feedback their accuracy. Instead, after both participants had submitted their first guess, they were shown their guess placed above their partner's guess (see Fig. 1). Both participants were then asked whether they wanted to change their own guess. If either participant in the dyad wanted to change their guess, that participant could do so (again with a 1-second time limit) while still being able to view their partner's previous guess. Participants who chose to keep their previously submitted guess were informed that their partner chose to submit a new guess and waited for their partner to finish. If both participants chose to change their guesses, they did so at approximately the same time---that is, immediately after they said that they wanted to change their guess and without being able to see their partner's decision or new guess. After the new guesses were submitted, both participants were again shown both guesses and were allowed to change or keep their guess. This process continued until both participants chose to keep their guess.

Participants were informed that their final accuracy bonus would only be calculated using their final guess. However, because they had no means to communicate with their partner about whether each would be accepting or changing their guesses, each participant could not have known whether their decision to keep the guess would have been their final guess for the trial. As a result, our statistical models use all guesses, not just final guesses (see next section for more detail).

All dyads completed 10 training trials (alone) and 15 test trials (with their partner). All training and test stimuli were randomly generated for each dyad, but both participants within the dyad were given the same stimuli. Stimuli were drawn from a uniform distribution between 1% and 100% (inclusive) of the total possible line length; this could have, by chance, resulted in some relatively easier stimulus sets for some dyads, which should be mitigated by our sample size. After participating, each participant completed a series of questionnaires about the game on a series of 1-10 Likert-style scales, including the perceived difficulty of the task, how engaged they were in the task, and questions about their own and their partner's cooperativeness and trustworthiness.

## Measures and Model Specifications

For clarity, we present the measures and model specifications together. Each measure used in one of our three models model is defined and written in bold the first time it is presented in this section.

### Model 1 Specifications: Do Partners' Perception and Memory Couple in Minimally Interacting Contexts?

Our first model tested whether individuals' ratings became more similar over time while accounting for training improvement. 

To do that, we first calculated each participant's **error** for *each guess* of *each trial*. Error was measured as a ratio relative to the total possible error on a given target stimulus trial. That is, rather than taking a given guess's error relative to the total line length, error was calculated as the maximum *possible* error. For example, if the target stimulus was 60 units long, participants could either under-estimate the line length by 60 or over-estimate it by 40. As such, the maximum possible error for that trial would be 60, and the participant's error would be calculated relative to that maximum possible error. We chose to use normalized error---rather than real error---as a measure of performance that natively controlled for the "possible wrong-ness" associated with any given line.

We then quantified perceptual and memory **coordination** (or how similar participants' perceptual and memory systems became over time) as the cross-correlation coefficient of participants' error. Cross-correlation---a common measure of coordination [@paxton2013argument]---was calculated using all guesses across all trials within a window of +/-5 guesses. Although cross-correlation produces information about leading and following behavior, we have no *a priori* expectations about which of the two participants would emerge as a leader (given they have no information about their partner nor any assigned roles). Our first-pass analyses therefore ignore any directionality by incorporating **absolute lag**, averaging across the each incremental lags (i.e., leading/following in both participants' directions).

To provide a baseline measure of **training improvement**, we calculated the slope of each participant's normalized error over all training trials. To account for individual differences in self-assessed task difficulty, we used ordinal **ratings of difficulty** that each participant gave after the task.

Our first model was a linear mixed-effects model predicting coordination of normalized error with absolute lag and training improvement as fixed effects, using dyad and difficulty ratings as random effects.

### Model 2 Specifications: Can We Identify Signatures of Learning *and* Coordinative Processes?

During the experiment, both participants are not simply influencing one another (as tested in Model 1)---but are also simultaneously learning to play the game. To ensure any similarity found by Model 1 would not be simply an artifact of both participants improving individually, we needed to test the relation between participants' (1) adaptation to their partner's perceptual estimation and memory and (2) own performance changed over time. If participants were adapting along both avenues, we could find evidence of these dual processes through differences in their rates of adaptation over time. 

To do this, we used the normalized error values (described above) to derive two measures. To answer the latter point, we used each participant's normalized error for *each guess* in *each trial* as their **true error**---in other words, how much the participant differed from the stimulus. To answer the former, we calculated the absolute difference between both participant's true error to obtain the **partner error** for *each guess* in *each trial*---or how much the participant's guess different from their partner's.

Because we are interested in understanding this process dynamically, we captured participants' progress over time by creating a **cumulative guess counter**, serving as a form of abstracted time spent engaging with one another and the experiment. While the measure of coordination in Model 1 presented a time-abstracted measure of coordination across the entire experiment, this model provides a snapshot of coordination in real time, measuring the learning and coordinative processes from guess to guess.

For Model 2, we built a linear mixed-effects model predicting the cumulative guess counter with each participant's true error, partner error, the interaction term between the two, and training improvement (described above) as fixed effects. We also included random effects for participant and difficulty. (We did not include dyad as a random effect in this model because the variance in the guesses occurred at the participant level.) 

### Model 3 Specifications: Do Social Factors Impact Coordination in Minimally Interacting Contexts?

To explore the role that social judgements can play even in minimally interactive contexts, our final model considered how trust might impact coordination. For this model, we captured a third measure of coordination: the participant's willingness to change their guess, which was captured by the **total number of guesses** that each participant submitted in each trial. 

Because participants individually chose whether to keep their previous guess or submit a new one while being able to see their partner's guess, we could expect that participants who trust their partner more would be more likely to change their guess---especially if there were large **absolute differences between the partner's first guesses**. Trust was measured as each participant's self-reported Likert-style **rating of their trust in their partner** ("How much do you feel you trusted your partner's opinion during the experiment?").

Model 3 was a linear-mixed effects model predicting the total number of guesses in a trial with fixed effects for their trust in their partner and for the difference in participants' first guess on that trial, while controlling for trial number and how much they improved during their own training. Model 3 also included participant and difficulty as random effects.

# Results

All analyses were performed in R [@r2016r]. Each model was built according to the model specifications described above. Linear mixed-effects models were performed using the `lme4` package [@bates2015fitting] using the maximal random slope structure for each random intercept to achieve model convergence [@barr2013random]. All main and interaction terms were centered and standardized prior to entry in the model, allowing the model estimates to be interpretable as effect sizes [@keith2005multiple].

While we do not have sufficient space in the current paper to provide model specifications for each of our three models, our code is fully and freely available in the GitHub repository for this project: http://www.github.com/a-paxton/perception-memory-coordination. Experiment data are available on the OSF repository for the project: https://osf.io/8fu7x/.

## Model 1 Results: Coordinated Error over Time

```{r build-model-1}

# create model
ccf_main_model = lmer(r ~ lag + training_improvement + 
                        (1 + lag + training_improvement | dyad) +
                        (1 + lag + training_improvement | difficulty),
                      data = ccf_st)

# make it readable
ccf_main_model_readable = xtable_lme(ccf_main_model)

# identify which rows correspond to which value
lag_row = 2
training_row = 3 

```

As predicted, we find that dyads were significantly and strongly coupled in their error ratings  (*\(\beta\)*=`r round(ccf_main_model_readable$Estimate[lag_row],2)`, *p*<`r ccf_main_model_readable$"p-value"[lag_row]+.0001`), with no effect of training improvement (*\(\beta\)*=`r round(ccf_main_model_readable$Estimate[training_row],2)`, *p*=`r ccf_main_model_readable$"p-value"[training_row]`). In other words, we find that players were more likely to produce lines with similar errors at the same time, even across repeated guesses within a single trial, regardless of how well-adapted they were to the task during training.

```{r build-model-2}

# difference the scores between own and partners' error
differences_in_error = info_plot %>%
  
  # start trials at zero and only include tests
  mutate(trial_number = trial_number-10) %>%
  dplyr::filter(trial_type=='test') %>%

  # take the absolute value of normalized error
  mutate(normalized_error = abs(normalized_error)) %>%
  mutate(partner_error = abs(partner_error)) %>%
    
  # create the error from partner variable and the difference between them
  mutate(error_from_partner = abs(normalized_error - partner_error)) %>%
  mutate(both.errors = error_from_partner * normalized_error) %>%
  
  # create error difference variable -- needed for Model 3 (not 2)
  mutate(error_diff = abs(normalized_error - error_from_partner)) %>%
  
  # use response-by-response numbering, not just trial-by-trial
  group_by(experiment, dyad, participant) %>%
  mutate(guess_number = 1:n()) %>%
  ungroup()

# standardize everything
differences_in_error_st = differences_in_error %>%
  mutate_all(funs(as.numeric(scale(as.numeric(.))))) %>%
  mutate_at(vars(participant,dyad),
            funs(factor)) %>%
  select(-trial_type,-network_id) %>%
  na.omit()

# run the model
checking_rates_model = lmer(guess_number ~ normalized_error + error_from_partner + 
                              both.errors + training_improvement + 
                              (1 + both.errors | participant) +
                              (1 + both.errors | difficulty),
                            data = differences_in_error_st)

# make it readable
checking_rates_model_readable = xtable_lme(checking_rates_model)

# identify which rows correspond to which value
normalized_error_row = 2
error_from_partner_row = 3 
both_errors_row = 4
training_row = 5

```

## Model 2 Results: Learning *and* Coordinative Processes

We found that participants' rates of adaptation to their partner significantly differed from their rates of adaptation to the game (*\(\beta\)*=`r round(checking_rates_model_readable$Estimate[error_from_partner_row],2)`, *p*<`r checking_rates_model_readable$"p-value"[error_from_partner_row]`; see Figure 2). In other words, this model suggests that individuals simultaneously engage in both learning *and* coordinative processes during the game, becoming attuned to the learning task while synchronizing to one another's cognitive processes.

Aside from the main effect of partner adaptation, no other predictors reached statistical significance (all *p*s>`r round(min(c(checking_rates_model_readable$"p-value"[normalized_error_row],checking_rates_model_readable$"p-value"[both_errors_row],checking_rates_model_readable$"p-value"[training_row])),2)`).

## Model 3 Results: Social Factors in Minimally Interacting Contexts

```{r build-model-3}

# get the maximum number of guess changes at each trial
social_parameters = differences_in_error %>% ungroup() %>%
  group_by(experiment, dyad, participant, trial_number) %>%
  dplyr::filter(guess_counter == max(guess_counter) |
                  guess_counter == min(guess_counter)) %>%
  ungroup()

# get the original guess and the last guess
first_guess = social_parameters %>%
  group_by(experiment, dyad, participant, trial_number) %>%
  dplyr::filter(guess_counter==min(guess_counter)) %>%
  ungroup() %>%
  select(experiment, dyad, participant, trial_number, error_diff) %>%
  rename('first_error_diff' = 'error_diff')

# merge into the original
social_parameters = social_parameters %>% ungroup() %>%
  group_by(experiment, dyad, participant, trial_number) %>%
  dplyr::filter(guess_counter==max(guess_counter)) %>%
  ungroup() %>%
  rename('final_error_diff' = 'error_diff') %>%
  left_join(., first_guess,
            by=c('experiment',
                 'dyad',
                 'participant',
                 'trial_number')) %>%
  
  # grab only one row
  distinct()

# standardize everything
social_parameters_st = social_parameters %>% ungroup() %>%
  mutate_all(funs(as.numeric(scale(as.numeric(.))))) %>%
  select(-trial_type,-network_id)

# build the model
trust_model = lmer(guess_counter ~ trust_partner + trial_number + first_error_diff + training_improvement +
                     (1 + trust_partner + trial_number | participant) +
                     (1 + trust_partner + trial_number | difficulty),
                   data = social_parameters_st)
trust_model_model_readable = xtable_lme(trust_model)

# get row info
trust_partner_row = 2
trial_number_row = 3
first_error_row = 4
training_row = 5

```

We found that greater trust in their partner predicted a small but statisically significant *increase* in the number of iterations of guesses within a trial (*\(\beta\)*=`r round(trust_model_model_readable$Estimate[trust_partner_row],2)`, *p*<`r trust_model_model_readable$"p-value"[trust_partner_row]`), although we found no difference in the number of guesses based solely on the differences in the partners' first guesses (*\(\beta\)*=`r round(trust_model_model_readable$Estimate[first_error_row],2)`, *p*=`r trust_model_model_readable$"p-value"[first_error_row]`). Ratings of partner trust were normally distributed around a mean of `r round(mean(social_parameters$trust_partner), 2)` (SD: `r round(sd(social_parameters$trust_partner), 2)`; range: `r min(social_parameters$trust_partner)`--`r max(social_parameters$trust_partner)`). In other words, although Models 1 and 2 showed people improving and becoming more similar to their partners across trials, we also find that participants are more willing to concede that their partner's guess was correct when the participant trusted their partner---regardless of how similar or different their first guess on the trial was.

Interestingly, we found that participants took more guesses on test trials when they improved more in their training trials (*\(\beta\)*=`r round(trust_model_model_readable$Estimate[training_row],2)`, *p*<`r trust_model_model_readable$"p-value"[training_row]`). Because those with the greatest training improvement would be the most poorly performing initial players (i.e., because high-performing individuals would have a much narrower band of possible improvement), this suggests that poorer-performing players are more likely to divide the cognitive labor of the task and follow the lead of their higher-performing partner.

We also saw an effect of trial (*\(\beta\)*=`r round(trust_model_model_readable$Estimate[trial_number_row],2)`, *p*<`r trust_model_model_readable$"p-value"[trial_number_row]`), indicating that people changed their guesses fewer times per trial as the game progressed. This could be an effect of learning (i.e., because both participants are improving and becoming more similar from trial to trial), of experiment fatigue (e.g., if participants simply want to end the game more quickly), or of some combination of the two.

```{r , fig.env = "figure", fig.pos = "H", fig.align='left', fig.width=3, fig.height=3, set.cap.width=T, num.cols.cap=1, fig.cap = "Difference over time in coordinative and learning processes, or the change in guess deviation from truth (in blue) and from their partner's guess (in red) over the game."}

# create a plot to show difference in slopes
partner_vs_truth_slope_plot = ggplot(differences_in_error,
                                     aes(x = trial_number,
                                         y = normalized_error)) +
  
  # smooth the from-truth error
  geom_smooth(method='lm',
              formula=y~x,
              aes(colour='True error')) +
  
  # smooth the from-partner error
  geom_smooth(aes(x = trial_number,
                  y = error_from_partner,
                  colour = 'Partner error'),
              method = 'lm',
              formula=y~x) +
  
  # label
  ylab('Normalized absolute\nerror of guess') +
  scale_x_continuous(breaks=c(1,5,10,15)) +
  xlab('Trial') +
  ggtitle('Participant true error\nand partner error over test trials') +
  theme(legend.position="bottom",
        legend.direction="horizontal",
        axis.title = element_text(size = 10),
        text = element_text(size = 10),
        plot.title = element_text(size = 10),
        legend.text=element_text(size=6),
        legend.title=element_text(size=7)) +
  scale_colour_discrete(name="Error source")

# save a high-resolution version of the plot
ggsave(plot = partner_vs_truth_slope_plot,
       height = 3,
       width = 5,
       filename = './figures/pmc-partner_vs_truth_slope_plot.jpg')

# save a smaller version of the plot for knitr
ggsave(plot = partner_vs_truth_slope_plot,
       height = 3,
       width = 5,
       dpi=100,
       filename = './figures/pmc-partner_vs_truth_slope_plot-knitr.jpg')

# plot
partner_vs_truth_slope_plot

```

# Discussion

Inspired by established lines of research on interpersonal coordination, we explored how minimally interactive contexts can shape the emergence of interpersonal dynamics. Using an online experiment that provided participants with only one means of communication---their partner's estimates---we found evidence of coordination of cognitive systems despite minimal social information and context.

As expected, there was low-level perceptual and memory coordination between players throughout the game. Congruent with findings about postural sway [@shockley2003mutual] and gaze [@richardson2005looking], the present work suggests that some behavioral and cognitive processes can become coordinated even when separated in time and space, given some access to the relevant process in another person and a task-based interactive context to which that process is essential. Individual learning and performance were unable to fully account for the players' similarity to one another.

Finally, like other work on coordination, we found that participants' coordination behaviors were shaped by subtle social judgements. Within coordination research, rapport has long been upheld as one of the important predictors of coordination [@hove2009s]. Similarly, we found that players' decisions to change their guesses were influenced by their self-reported ratings of their partner's trustworthiness.

Taken together, our findings contribute to the ongoing efforts to understand the form, function, and emergence of coordination. We were specifically interested in pursuing three important questions around interpersonal coordination of perception and memory: whether it emerges during minimally interactive contexts, whether it can be distinguished from other contemporaneous behavioral and cognitive processes, and whether it is influenced by subtle social judgements. In addition to these theoretical contributions, we also hope to have provided an example of the utility of crowdsourcing platforms to investigate core principles of interpersonal coordination and human interaction at a larger scale without relinquishing experimental control.

## Future Directions

Some of the questions left open by the present study may provide interesting avenues for future work, both for better understanding some of the effects identified here and for extending them into novel territory.

We found that participants were more likely to change their guesses during the game when playing with a partner who they rated trustworthy *after* the game. One plausible explanation for this behavior may be that participants were more willing to change their own guess to be closer to the guess provided by a partner whose perceptual and memory capabilities they trusted. Another (not necessarily exclusive) possibility is that feelings of trust are built throughout the game: Since the ratings were made at the end of the experiment, it may be that the partner's behavior throughout the interaction---*aside* from accuracy (e.g., duration of the interaction)---builds to create that feeling of trust. Future work should explore the degree to which these explanations may (individually or together) explain the emergent social phenomena observed even minimally interactive contexts.

Second, we explored trustworthiness as a social construct during a task that allowed only minimal participation between participants. The trustworthiness measure was intentionally asked to allow participants to answer broadly, providing a signal of the latent social information that participants constructed when *only* their partner's task-related behavior was available to them. While we see their responses as a signal of very subtle social forces, we readily recognize that these ratings of trustworthiness may have been influenced by the individual's own confidence or ability---as many social judgements are often influenced by the assessor's own characteristics. Future work should expand on this to explore the interplay between individual and interpersonal assessments.

<!-- Second, we found that participants became coordinated even on a relatively short timescale, with the average dyad taking `r round(mean(participation_descriptives$duration),2)` minutes to complete the game. Future work could examine hysteresis effects in the individual participant's perceptual and memory systems by pairing two participants from different dyads with one another to see how long it takes before they begin to form new coordination patterns. With most research focusing on the initial creation of coordination, understanding the flexibility of these dissipative interpersonal coordinative structures would provide new insight into this phenomenon. -->

Finally, given the simplicity of this experimental design and the broad participant pools available through crowdsourcing sites, this work could be expanded to examine other important questions of scale in interpersonal coordination. The majority of research on interpersonal coordination has tended to focus on dyadic interaction, as we did here, but many real-world social settings include more than two people. Accordingly, any coherent theory around interaction must be able to account for such settings. Crowdsourcing and real-time social experiments enable researchers to control the interaction space much more tightly, enabling the targeted focus on specific processes across a massive potential participant population.

# Acknowledgements

Our thanks go to Nelle Varoquaux and to the Dallinger development team for their assistance in debugging the experiment. This work was funded in part by DARPA Cooperative Agreement D17AC00004, a Moore-Sloan Data Science Environments Fellowship to AP, the Gordon and Betty Moore Foundation through Grant GBMF3834, and the Alfred P. Sloan Foundation through Grant 2013-10-27 to the University of California, Berkeley.

# References 

```{r references}
# References will be generated automatically by Pandoc and included here.
# The following code is some latex to format the bibliography. Do not remove it.
```

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent